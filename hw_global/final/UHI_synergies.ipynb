{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc6087f5cd7c2b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a208e90a97858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  Load  and prepare HW and NO_HW df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5832fe3ddff1d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d24e7a57321c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the parquet data from /Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet\n",
    "#data_dir = '/Users/yguo/DataSpellProjects/hw/data/parquet'\n",
    "data_dir = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet'\n",
    "file_name_hw = 'HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "hw_path = os.path.join(data_dir, file_name_hw)\n",
    "df_hw = pd.read_parquet(hw_path)\n",
    "print(df_hw.info())\n",
    "df_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6505094eaed4966",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  add hour, month and year to the df_hw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53304be825b6ed52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure 'time' is of datetime type\n",
    "df_hw.index = df_hw.index.set_levels([df_hw.index.levels[0], df_hw.index.levels[1], pd.to_datetime(df_hw.index.levels[2])])\n",
    "\n",
    "# Extract hour, month and year from 'time'\n",
    "df_hw['hour'] = df_hw.index.get_level_values('time').hour\n",
    "df_hw['month'] = df_hw.index.get_level_values('time').month\n",
    "df_hw['year'] = df_hw.index.get_level_values('time').year\n",
    "df_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa0b6bd788586a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Group by 'lat', 'lon', 'year', 'month', and 'hour', then calculate the mean of 'UHI' and 'UBWI'\n",
    "# df_hw_avg = df_hw.groupby(['lat', 'lon', 'year', 'month', 'hour']).mean()\n",
    "# df_hw_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a51d9af5448cb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ee0d5a634867e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "file_name_no_hw = 'NO_HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "no_hw_path = os.path.join(data_dir, file_name_no_hw)\n",
    "df_no_hw = pd.read_parquet(no_hw_path)\n",
    "print(df_no_hw.info())\n",
    "df_no_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baf241ca6cd1bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#todo: add UHI and NO_HW UHI to make sure they are the same as the oringal netcdf data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a2dfcd82835f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Validate there is not overlap between the HW and NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41e23a1df7c37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the key for both df_hw and df_no_hw are lat, lon and time. please show python code that they don't overlap on those keys\n",
    "# Convert the MultiIndex of both DataFrames to sets\n",
    "# keys_hw = set(df_hw.index)\n",
    "# keys_no_hw = set(df_no_hw.index)\n",
    "# \n",
    "# # Check if the intersection of these sets is empty\n",
    "# overlap = keys_hw & keys_no_hw\n",
    "# \n",
    "# # If the intersection is empty, print that there is no overlap. Otherwise, print the overlapping keys.\n",
    "# if not overlap:\n",
    "#     print(\"There is no overlap between the keys of df_hw and df_no_hw.\")\n",
    "# else:\n",
    "#     print(\"The following keys overlap between df_hw and df_no_hw:\")\n",
    "#     print(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fff9f973f6840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# group df_no_hw by lat, lon, year and hour of the day avaerage UHI and UBWI\n",
    "df_no_hw.index = df_no_hw.index.set_levels(\n",
    "    [df_no_hw.index.levels[0], df_no_hw.index.levels[1], pd.to_datetime(df_no_hw.index.levels[2])])\n",
    "df_no_hw['hour'] = df_no_hw.index.get_level_values('time').hour\n",
    "df_no_hw['year'] = df_no_hw.index.get_level_values('time').year\n",
    "df_no_hw_avg = df_no_hw.groupby(['lat', 'lon', 'year', 'hour']).mean()\n",
    "df_no_hw_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236c292b975adc7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  2: Calculate the difference between UHI in df_hw and df_no_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26fbc35ae4870b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  UHI HW - NO_HW ( HW hour data - NO_HW yearl average data for the hour) \n",
    "the df_no_hw_avg is the average value for a given hour of the day throughout the year.\n",
    "In the 2018 Zhao paper they seem to just do average the whole 30 years. \n",
    "I want to substract the average UHI on the given hour for a given year from the hourly UHI data I have in df_hw, matching the year and hour between the two dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d41a33572272e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.1: Reset the index of df_hw and df_no_hw_avg (be careful on the increased memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcfa232880cb50",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hw.info()\n",
    "df_hw_reset = df_hw.reset_index()\n",
    "df_hw_reset.info()\n",
    "df_no_hw_avg.info()\n",
    "df_no_hw_avg_reset = df_no_hw_avg.reset_index()\n",
    "df_no_hw_avg_reset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9ed8b47547a8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##   Step 2.2: Merge df_hw with df_no_hw_avg_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235cd40864887ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(df_hw_reset, df_no_hw_avg_reset[['lat', 'lon', 'year', 'hour', 'UHI', 'UBWI']],\n",
    "                     on=['lat', 'lon', 'year', 'hour'],\n",
    "                     suffixes=('', '_avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b425e5780987917",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ac989c16eea46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.3: Subtract the average UHI from the hourly UHI and store in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44408a202722573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df['UHI_diff'] = merged_df['UHI'] - merged_df['UHI_avg']\n",
    "merged_df['UBWI_diff'] = merged_df['UBWI'] - merged_df['UBWI_avg']\n",
    "# Now, merged_df contains your original data along with the subtracted UHI values in 'UHI_diff'\n",
    "merged_df  # To check the first few rows of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463f2a5168f843e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df[['UHI_diff', 'UBWI_diff']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8948bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99696a2c4ea4ab9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2.4: Validate the results by checking the UHI values for a specific location and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e223411d549cb4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "row_index = 198\n",
    "print(merged_df.iloc[row_index].UHI_avg)\n",
    "a_row = merged_df.iloc[row_index]\n",
    "df_no_hw_avg_reset.loc[(df_no_hw_avg_reset['lat'] == a_row.lat) & (df_no_hw_avg_reset['lon'] == a_row.lon) & (\n",
    "                df_no_hw_avg_reset['year'] == a_row.year) & (\n",
    "                df_no_hw_avg_reset['hour'] == a_row.hour)].UHI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdb0d3",
   "metadata": {},
   "source": [
    "#  3: Averaged data for each local hour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9969b76",
   "metadata": {},
   "source": [
    "##  Step 3.1: Adjust to local hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_time_to_local_and_add_hour(df):\n",
    "    \"\"\"\n",
    "    Converts the UTC timestamp in the DataFrame to local time based on longitude and adds a column for the local hour.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns for latitude ('lat'), longitude ('lon'), and UTC timestamp ('time')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns ('local_time' and 'local_hour') for the timestamp adjusted to local time and the hour extracted from the local time\n",
    "    \"\"\"\n",
    "    # Function to calculate timezone offset from longitude\n",
    "    def calculate_timezone_offset(longitude):\n",
    "        return np.floor(longitude / 15.0).astype(int)  # Approximate, not accounting for DST or specific timezone rules\n",
    "\n",
    "    # Calculate timezone offsets for each row based on longitude\n",
    "    offsets = calculate_timezone_offset(df['lon'].values)\n",
    "\n",
    "    # Adjust timestamps by the offsets\n",
    "    df['local_time'] = df['time'] + pd.to_timedelta(offsets, unit='h')\n",
    "\n",
    "    # Extract the hour from the 'local_time' and create a new column\n",
    "    df['local_hour'] = df['local_time'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Assuming 'df' is your original DataFrame\n",
    "# # Make sure 'time' column is in datetime format\n",
    "# df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# # Convert UTC times to local times based on longitude and add local hour\n",
    "# df = convert_time_to_local_and_add_hour(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a90d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = convert_time_to_local_and_add_hour(merged_df)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ff8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea995b",
   "metadata": {},
   "source": [
    "##  Step 3.2 compute average based on local hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Group by 'lat', 'lon', and 'local_hour', then calculate the mean for 'UHI_diff'\n",
    "# Ensure grouped data is sorted by 'lat' and 'lon' before pivoting\n",
    "grouped_sorted = merged_df.groupby(['lat', 'lon', 'local_hour'])['UHI_diff'].mean().reset_index().sort_values(by=['lat', 'lon', 'local_hour'])\n",
    "\n",
    "grouped_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the grouped and sorted data into an xarray Dataset for each local_hour\n",
    "xarrays = {}\n",
    "for hour in grouped_sorted['local_hour'].unique():\n",
    "    # Filter the grouped DataFrame for the current local_hour\n",
    "    hourly_data = grouped_sorted[grouped_sorted['local_hour'] == hour]\n",
    "    \n",
    "    # Ensure the data is sorted\n",
    "    hourly_data = hourly_data.sort_values(by=['lat', 'lon'])\n",
    "    \n",
    "    # Pivot the data to create a 2D array of 'UHI_diff' values\n",
    "    uhi_diff_grid = hourly_data.pivot(index='lat', columns='lon', values='UHI_diff').values\n",
    "\n",
    "    # # Ensure there are no NaN values, which can cause issues with plotting\n",
    "    # if np.isnan(uhi_diff_grid).any():\n",
    "    #     print(f\"Hour {hour} contains NaN values and will be skipped.\")\n",
    "    #     continue\n",
    "    \n",
    "    lat_unique = hourly_data['lat'].unique()\n",
    "    lon_unique = hourly_data['lon'].unique()\n",
    "\n",
    "    # Create an xarray DataArray with sorted coordinates\n",
    "    xarrays[hour] = xr.DataArray(\n",
    "        uhi_diff_grid,\n",
    "        coords=[('lat', lat_unique), ('lon', lon_unique)],\n",
    "        dims=['lat', 'lon'],\n",
    "        name='UHI_diff'\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f22336",
   "metadata": {},
   "source": [
    "# 4: Plot and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_to_plot = 1\n",
    "xr_data = xarrays.get(hour_to_plot)\n",
    "\n",
    "if xr_data is not None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.set_title(f'UHI_diff Average for Local Hour: {hour_to_plot}')\n",
    "    ax.coastlines()\n",
    "\n",
    "    # Sort the DataArray if it's not already sorted\n",
    "    xr_data_sorted = xr_data.sortby(['lat', 'lon'])\n",
    "    \n",
    "    im = xr_data_sorted.plot(ax=ax, add_colorbar=True, cmap='viridis', transform=ccrs.PlateCarree())\n",
    "    im.colorbar.set_label('Average UHI_diff')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data available for local_hour {hour_to_plot}\")\n",
    "\n",
    "xr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37646c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d0e1f89c5637f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def convert_index_time_to_local(df):\n",
    "#     \"\"\"\n",
    "#     Converts the UTC timestamp in the DataFrame's MultiIndex to local time based on longitude.\n",
    "    \n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): DataFrame with a MultiIndex (latitude, longitude, UTC timestamp)\n",
    "    \n",
    "#     Returns:\n",
    "#     pd.DataFrame: DataFrame with the timestamp in the MultiIndex adjusted to local time\n",
    "#     \"\"\"\n",
    "#     # Function to calculate timezone offset from longitude\n",
    "#     def calculate_timezone_offset(longitude):\n",
    "#         return np.floor(longitude / 15.0).astype(int)  # Approximate, not accounting for DST or specific timezone rules\n",
    "\n",
    "#     # Extract MultiIndex levels\n",
    "#     latitudes, longitudes, timestamps = zip(*df.index)\n",
    "\n",
    "#     # Convert to arrays for vectorized operations\n",
    "#     longitudes = np.array(longitudes)\n",
    "#     timestamps = pd.to_datetime(list(timestamps))\n",
    "\n",
    "#     # Calculate timezone offsets\n",
    "#     offsets = calculate_timezone_offset(longitudes)\n",
    "\n",
    "#     # Adjust timestamps by the offsets\n",
    "#     adjusted_timestamps = [timestamp + pd.Timedelta(hours=offset) for timestamp, offset in zip(timestamps, offsets)]\n",
    "\n",
    "#     # Create a new MultiIndex with the adjusted timestamps\n",
    "#     new_index = pd.MultiIndex.from_arrays([latitudes, longitudes, adjusted_timestamps], names=['Latitude', 'Longitude', 'LocalTime'])\n",
    "\n",
    "#     # Assign the new MultiIndex to the DataFrame and return it\n",
    "#     df.index = new_index\n",
    "#     return df\n",
    "\n",
    "# Usage example:\n",
    "# new_df = convert_index_time_to_local(df)\n",
    "# This will return a DataFrame with the index timestamps adjusted to local time based on longitude\n",
    "# Note: ONLY CALL THIS FUNCTION ON THE DATAFRAME WITH THE MULTIINDEX (latitude, longitude, UTC timestamp), ALSO ONLY CALL AT THE FINAL STEP(AFTER ALL AGGREATION DONE!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114062a4f714b96b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hw = convert_index_time_to_local(df_hw)\n",
    "df_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf239f91ba7eef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_count = df_hw.groupby(['lat', 'lon']).size()\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55ac33c9e875f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = \"/home/jguo/anaconda3/envs/I2000/share/proj\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "df_hw = pd.read_parquet('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet/HW_1985_1994.parquet')\n",
    "df_count = df_hw.groupby(['lat', 'lon']).size()\n",
    "df_count_sorted = df_count.sort_values(ascending=False).head(50)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "# Add gridlines\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "lats, lons = zip(*df_count_sorted.index)\n",
    "ax.scatter(lons, lats, color='red', transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2b4011aaa6e2a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df_count_sorted, geometry=gpd.points_from_xy(lons, lats))\n",
    "\n",
    "# Load Natural Earth dataset with coastlines\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Plot the world (coastlines)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "world.boundary.plot(ax=ax, color='black')\n",
    "\n",
    "# Plot the GeoDataFrame\n",
    "gdf.plot(marker='o', color='red', markersize=5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cd07ee4681ae7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_count_sorted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dd7807065fff7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "\n",
    "# Create a plot with coastlines\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "\n",
    "# Extract latitudes, longitudes, and data values from the MultiIndex Series\n",
    "latitudes = df_count_sorted.index.get_level_values(0)\n",
    "longitudes = df_count_sorted.index.get_level_values(1) % 360  # Adjust longitudes if they are in a 0-360 format\n",
    "data_values = df_count_sorted.values\n",
    "\n",
    "# Plot the data\n",
    "scatter = plt.scatter(longitudes, latitudes, c=data_values, cmap='viridis', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add gridlines and labels\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(scatter, orientation='vertical', pad=0.02, aspect=50)\n",
    "cbar.set_label('Data Value')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5ebebd98351d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the Series with a MultiIndex to an xarray DataArray\n",
    "da = xr.DataArray(df_count_sorted, dims=['lat_lon'], name='data')\n",
    "da['lat'] = ('lat_lon', df_count_sorted.index.get_level_values('lat'))\n",
    "da['lon'] = ('lat_lon', df_count_sorted.index.get_level_values('lon'))\n",
    "# Define the new grid (adjust the resolution as needed)\n",
    "new_lat = np.linspace(da['lat'].min(), da['lat'].max(), num=100)\n",
    "new_lon = np.linspace(da['lon'].min(), da['lon'].max(), num=100)\n",
    "\n",
    "# Interpolate the DataArray onto the new grid\n",
    "da_interp = da.interp(lat=new_lat, lon=new_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0569166f164a03a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da.HW_Count.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38124c98965eac9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_no_hw = pd.read_parquet('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet/NO_HW_1985_1994.parquet')\n",
    "\n",
    "print(df_no_hw.info())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced860dc34abd47",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hw.index.levels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d667d9b9203493",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure 'time' is of datetime type\n",
    "df_hw.index = df_hw.index.set_levels([df_hw.index.levels[0], df_hw.index.levels[1], pd.to_datetime(df_hw.index.levels[2])])\n",
    "\n",
    "# Extract hour, month and year from 'time'\n",
    "df_hw['hour'] = df_hw.index.get_level_values('time').hour\n",
    "df_hw['month'] = df_hw.index.get_level_values('time').month\n",
    "df_hw['year'] = df_hw.index.get_level_values('time').year\n",
    "\n",
    "# Group by 'lat', 'lon', 'year', 'month', and 'hour', then calculate the mean of 'UHI' and 'UBWI'\n",
    "df_avg = df_hw.groupby(['lat', 'lon', 'year', 'month', 'hour']).mean()\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b4b82e2e5b73f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
