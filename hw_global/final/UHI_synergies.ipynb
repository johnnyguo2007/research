{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc6087f5cd7c2b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a208e90a97858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  Load  and prepare HW and NO_HW df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5832fe3ddff1d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d24e7a57321c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the parquet data from /Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet\n",
    "#data_dir = '/Users/yguo/DataSpellProjects/hw/data/parquet'\n",
    "data_dir = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet'\n",
    "file_name_hw = 'HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "hw_path = os.path.join(data_dir, file_name_hw)\n",
    "df_hw = pd.read_parquet(hw_path)\n",
    "print(df_hw.info())\n",
    "df_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6505094eaed4966",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  add hour, month and year to the df_hw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53304be825b6ed52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure 'time' is of datetime type\n",
    "df_hw.index = df_hw.index.set_levels([df_hw.index.levels[0], df_hw.index.levels[1], pd.to_datetime(df_hw.index.levels[2])])\n",
    "\n",
    "# Extract hour, month and year from 'time'\n",
    "df_hw['hour'] = df_hw.index.get_level_values('time').hour\n",
    "df_hw['month'] = df_hw.index.get_level_values('time').month\n",
    "df_hw['year'] = df_hw.index.get_level_values('time').year\n",
    "df_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa0b6bd788586a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Group by 'lat', 'lon', 'year', 'month', and 'hour', then calculate the mean of 'UHI' and 'UBWI'\n",
    "# df_hw_avg = df_hw.groupby(['lat', 'lon', 'year', 'month', 'hour']).mean()\n",
    "# df_hw_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a51d9af5448cb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ee0d5a634867e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "file_name_no_hw = 'NO_HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "no_hw_path = os.path.join(data_dir, file_name_no_hw)\n",
    "df_no_hw = pd.read_parquet(no_hw_path)\n",
    "print(df_no_hw.info())\n",
    "df_no_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baf241ca6cd1bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#todo: add UHI and NO_HW UHI to make sure they are the same as the oringal netcdf data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a2dfcd82835f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Validate there is not overlap between the HW and NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41e23a1df7c37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the key for both df_hw and df_no_hw are lat, lon and time. please show python code that they don't overlap on those keys\n",
    "# Convert the MultiIndex of both DataFrames to sets\n",
    "# keys_hw = set(df_hw.index)\n",
    "# keys_no_hw = set(df_no_hw.index)\n",
    "# \n",
    "# # Check if the intersection of these sets is empty\n",
    "# overlap = keys_hw & keys_no_hw\n",
    "# \n",
    "# # If the intersection is empty, print that there is no overlap. Otherwise, print the overlapping keys.\n",
    "# if not overlap:\n",
    "#     print(\"There is no overlap between the keys of df_hw and df_no_hw.\")\n",
    "# else:\n",
    "#     print(\"The following keys overlap between df_hw and df_no_hw:\")\n",
    "#     print(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fff9f973f6840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# group df_no_hw by lat, lon, year and hour of the day avaerage UHI and UBWI\n",
    "df_no_hw.index = df_no_hw.index.set_levels(\n",
    "    [df_no_hw.index.levels[0], df_no_hw.index.levels[1], pd.to_datetime(df_no_hw.index.levels[2])])\n",
    "df_no_hw['hour'] = df_no_hw.index.get_level_values('time').hour\n",
    "df_no_hw['year'] = df_no_hw.index.get_level_values('time').year\n",
    "df_no_hw_avg = df_no_hw.groupby(['lat', 'lon', 'year', 'hour']).mean()\n",
    "df_no_hw_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236c292b975adc7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  2: Calculate the difference between UHI in df_hw and df_no_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26fbc35ae4870b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  UHI HW - NO_HW ( HW hour data - NO_HW yearl average data for the hour) \n",
    "the df_no_hw_avg is the average value for a given hour of the day throughout the year.\n",
    "In the 2018 Zhao paper they seem to just do average the whole 30 years. \n",
    "I want to substract the average UHI on the given hour for a given year from the hourly UHI data I have in df_hw, matching the year and hour between the two dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d41a33572272e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.1: Reset the index of df_hw and df_no_hw_avg (be careful on the increased memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcfa232880cb50",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hw.info()\n",
    "df_hw_reset = df_hw.reset_index()\n",
    "df_hw_reset.info()\n",
    "df_no_hw_avg.info()\n",
    "df_no_hw_avg_reset = df_no_hw_avg.reset_index()\n",
    "df_no_hw_avg_reset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9ed8b47547a8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##   Step 2.2: Merge df_hw with df_no_hw_avg_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235cd40864887ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(df_hw_reset, df_no_hw_avg_reset[['lat', 'lon', 'year', 'hour', 'UHI', 'UBWI']],\n",
    "                     on=['lat', 'lon', 'year', 'hour'],\n",
    "                     suffixes=('', '_avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b425e5780987917",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ac989c16eea46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.3: Subtract the average UHI from the hourly UHI and store in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44408a202722573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df['UHI_diff'] = merged_df['UHI'] - merged_df['UHI_avg']\n",
    "merged_df['UBWI_diff'] = merged_df['UBWI'] - merged_df['UBWI_avg']\n",
    "# Now, merged_df contains your original data along with the subtracted UHI values in 'UHI_diff'\n",
    "merged_df  # To check the first few rows of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463f2a5168f843e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df[['UHI_diff', 'UBWI_diff']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8948bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99696a2c4ea4ab9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2.4: Validate the results by checking the UHI values for a specific location and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e223411d549cb4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "row_index = 198\n",
    "print(merged_df.iloc[row_index].UHI_avg)\n",
    "a_row = merged_df.iloc[row_index]\n",
    "df_no_hw_avg_reset.loc[(df_no_hw_avg_reset['lat'] == a_row.lat) & (df_no_hw_avg_reset['lon'] == a_row.lon) & (\n",
    "                df_no_hw_avg_reset['year'] == a_row.year) & (\n",
    "                df_no_hw_avg_reset['hour'] == a_row.hour)].UHI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdb0d3",
   "metadata": {},
   "source": [
    "#  3: Averaged data for each local hour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9969b76",
   "metadata": {},
   "source": [
    "##  Step 3.1: Adjust to local hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_time_to_local_and_add_hour(df):\n",
    "    \"\"\"\n",
    "    Converts the UTC timestamp in the DataFrame to local time based on longitude and adds a column for the local hour.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns for latitude ('lat'), longitude ('lon'), and UTC timestamp ('time')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns ('local_time' and 'local_hour') for the timestamp adjusted to local time and the hour extracted from the local time\n",
    "    \"\"\"\n",
    "    # Function to calculate timezone offset from longitude\n",
    "    def calculate_timezone_offset(longitude):\n",
    "        return np.floor(longitude / 15.0).astype(int)  # Approximate, not accounting for DST or specific timezone rules\n",
    "\n",
    "    # Calculate timezone offsets for each row based on longitude\n",
    "    offsets = calculate_timezone_offset(df['lon'].values)\n",
    "\n",
    "    # Adjust timestamps by the offsets\n",
    "    df['local_time'] = df['time'] + pd.to_timedelta(offsets, unit='h')\n",
    "\n",
    "    # Extract the hour from the 'local_time' and create a new column\n",
    "    df['local_hour'] = df['local_time'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Assuming 'df' is your original DataFrame\n",
    "# # Make sure 'time' column is in datetime format\n",
    "# df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# # Convert UTC times to local times based on longitude and add local hour\n",
    "# df = convert_time_to_local_and_add_hour(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a90d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = convert_time_to_local_and_add_hour(merged_df)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ff8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea995b",
   "metadata": {},
   "source": [
    "##  Step 3.2 compute average based on local hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJ_LIB\"] = \"/home/jguo/anaconda3/envs/I2000/share/proj\"\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Group by 'lat', 'lon', and 'local_hour', then calculate the mean for 'UHI_diff'\n",
    "# Ensure grouped data is sorted by 'lat' and 'lon' before pivoting\n",
    "grouped_sorted = merged_df.groupby(['lat', 'lon', 'local_hour'])['UHI_diff'].mean().reset_index().sort_values(by=['lat', 'lon', 'local_hour'])\n",
    "\n",
    "grouped_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grouped_sorted.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3cfbaf16ab84993",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29f22336",
   "metadata": {},
   "source": [
    "# 4: Plot and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83c80de8d0e6fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'grouped_sorted' is your DataFrame\n",
    "\n",
    "# Function to normalize longitude values to the range [-180, 180]\n",
    "def normalize_longitude(lon):\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'grouped_sorted' is your DataFrame\n",
    "\n",
    "# Function to normalize longitude values to the range [-180, 180]\n",
    "def normalize_longitude(lon):\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "# Normalize the longitude values in your DataFrame\n",
    "grouped_sorted['lon'] = grouped_sorted['lon'].apply(normalize_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the map drawing function for subplots\n",
    "def draw_map_subplot(m, ax):\n",
    "    m.drawcoastlines(linewidth=0.5, ax=ax)\n",
    "    m.drawcountries(linewidth=0.5, ax=ax)\n",
    "    m.fillcontinents(color='coral', lake_color='aqua', alpha=0.3, ax=ax)\n",
    "    m.drawmapboundary(fill_color='aqua', ax=ax)\n",
    "    m.drawparallels(np.arange(-90., 91., 30.), labels=[1, 0, 0, 0], fontsize=10, ax=ax)\n",
    "    m.drawmeridians(np.arange(-180., 181., 60.), labels=[0, 0, 0, 1], fontsize=10, ax=ax)\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "n_hours = len(grouped_sorted['local_hour'].unique())\n",
    "n_rows = (n_hours + 2) // 3  # Adding 2 to ensure rounding up if there's a remainder\n",
    "\n",
    "# Create a figure to hold all subplots\n",
    "fig, axs = plt.subplots(n_rows, 3, figsize=(18, n_rows * 6), constrained_layout=True)\n",
    "\n",
    "# Iterate through each local_hour to create subplots\n",
    "for i, hour in enumerate(grouped_sorted['local_hour'].unique()):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = axs[row, col] if n_rows > 1 else axs[col]  # Adjust for the case of a single row\n",
    "\n",
    "    # Setup the Basemap\n",
    "    m = Basemap(projection='cyl', resolution='l', lat_0=0, lon_0=0, ax=ax)\n",
    "\n",
    "    draw_map_subplot(m, ax)\n",
    "\n",
    "    # Filter data for the current hour\n",
    "    df_hour = grouped_sorted[grouped_sorted['local_hour'] == hour]\n",
    "\n",
    "    # Scatter UHI_diff data\n",
    "    x, y = m(df_hour['lon'].values, df_hour['lat'].values)\n",
    "    sc = m.scatter(x, y, c=df_hour['UHI_diff'], cmap='hot', marker='o', edgecolor='none', alpha=0.75, ax=ax)\n",
    "\n",
    "    ax.set_title(f'UHI Difference Map at Local Hour {hour}')\n",
    "\n",
    "    # Add color bar to each subplot\n",
    "    plt.colorbar(sc, ax=ax, fraction=0.046, pad=0.04, label='UHI_diff')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {},
   "id": "bc9c690b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37646c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Miscalanous items"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8590a7c15580328f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf239f91ba7eef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_count = df_hw.groupby(['lat', 'lon']).size()\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55ac33c9e875f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "df_hw = pd.read_parquet('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet/HW_1985_1994.parquet')\n",
    "df_count = df_hw.groupby(['lat', 'lon']).size()\n",
    "df_count_sorted = df_count.sort_values(ascending=False).head(50)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "# Add gridlines\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "lats, lons = zip(*df_count_sorted.index)\n",
    "ax.scatter(lons, lats, color='red', transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b4b82e2e5b73f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## group by north south hemisphere"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2bfe88e76bdfdb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grouped_sorted.groupby(lambda x: (grouped_sorted.loc[x, 'lat'] > 0, grouped_sorted.loc[x, 'local_hour']))['UHI_diff'].mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5adcb30c2eb21982",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
