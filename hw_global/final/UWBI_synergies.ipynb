{
 "cells": [
  {
   "cell_type": "code",
   "id": "adcc6087f5cd7c2b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd4a208e90a97858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  Load  and prepare HW and NO_HW df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5832fe3ddff1d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load HW data"
   ]
  },
  {
   "cell_type": "code",
   "id": "d37d24e7a57321c5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the parquet data from /Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet\n",
    "#data_dir = '/Users/yguo/DataSpellProjects/hw/data/parquet'\n",
    "data_dir = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet'\n",
    "file_name_hw = 'HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "hw_path = os.path.join(data_dir, file_name_hw)\n",
    "df_hw = pd.read_parquet(hw_path)\n",
    "print(df_hw.info())\n",
    "df_hw"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6505094eaed4966",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Add hour, month and year to the df_hw\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c3eb77733a9f4eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ensure 'time' is of datetime type\n",
    "# converting the third level of the DataFrame's MultiIndex to datetime format, while keeping the other levels unchanged.\n",
    "df_hw.index = df_hw.index.set_levels([df_hw.index.levels[0], df_hw.index.levels[1], pd.to_datetime(df_hw.index.levels[2])])\n",
    "# df_hw.index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Extract hour, month and year from 'time'\n",
    "df_hw['hour'] = df_hw.index.get_level_values('time').hour\n",
    "df_hw['month'] = df_hw.index.get_level_values('time').month\n",
    "df_hw['year'] = df_hw.index.get_level_values('time').year\n",
    "df_hw"
   ],
   "id": "53304be825b6ed52",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3aa0b6bd788586a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# # Group by 'lat', 'lon', 'year', 'month', and 'hour', then calculate the mean of 'UHI' and 'UBWI'\n",
    "# df_hw_avg = df_hw.groupby(['lat', 'lon', 'year', 'month', 'hour']).mean()\n",
    "# df_hw_avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57a51d9af5448cb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Load NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e8ee0d5a634867e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "file_name_no_hw = 'NO_HW_1985_1994.parquet'\n",
    "#join data_dir and file_name\n",
    "no_hw_path = os.path.join(data_dir, file_name_no_hw)\n",
    "df_no_hw = pd.read_parquet(no_hw_path)\n",
    "print(df_no_hw.info())\n",
    "df_no_hw"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4baf241ca6cd1bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#todo: add UHI and NO_HW UHI to make sure they are the same as the oringal netcdf data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c0a2dfcd82835f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Validate there is not overlap between the HW and NO_HW data"
   ]
  },
  {
   "cell_type": "code",
   "id": "37a41e23a1df7c37",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#the key for both df_hw and df_no_hw are lat, lon and time. please show python code that they don't overlap on those keys\n",
    "# Convert the MultiIndex of both DataFrames to sets\n",
    "# keys_hw = set(df_hw.index)\n",
    "# keys_no_hw = set(df_no_hw.index)\n",
    "# \n",
    "# # Check if the intersection of these sets is empty\n",
    "# overlap = keys_hw & keys_no_hw\n",
    "# \n",
    "# # If the intersection is empty, print that there is no overlap. Otherwise, print the overlapping keys.\n",
    "# if not overlap:\n",
    "#     print(\"There is no overlap between the keys of df_hw and df_no_hw.\")\n",
    "# else:\n",
    "#     print(\"The following keys overlap between df_hw and df_no_hw:\")\n",
    "#     print(overlap)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac3fff9f973f6840",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# group df_no_hw by lat, lon, year and hour of the day avaerage UHI and UBWI\n",
    "df_no_hw.index = df_no_hw.index.set_levels(\n",
    "    [df_no_hw.index.levels[0], df_no_hw.index.levels[1], pd.to_datetime(df_no_hw.index.levels[2])])\n",
    "df_no_hw['hour'] = df_no_hw.index.get_level_values('time').hour\n",
    "df_no_hw['year'] = df_no_hw.index.get_level_values('time').year\n",
    "df_no_hw_avg = df_no_hw.groupby(['lat', 'lon', 'year', 'hour']).mean()\n",
    "df_no_hw_avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e236c292b975adc7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  2: Calculate the difference between UHI in df_hw and df_no_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26fbc35ae4870b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  UHI HW - NO_HW ( HW hour data - NO_HW yearl average data for the hour) \n",
    "the df_no_hw_avg is the average value for a given hour of the day throughout the year.\n",
    "In the 2018 Zhao paper they seem to just do average the whole 30 years. \n",
    "I want to substract the average UHI on the given hour for a given year from the hourly UHI data I have in df_hw, matching the year and hour between the two dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d41a33572272e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.1: Reset the index of df_hw and df_no_hw_avg (be careful on the increased memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ebcfa232880cb50",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_hw.info()\n",
    "df_hw_reset = df_hw.reset_index()\n",
    "df_hw_reset.info()\n",
    "df_no_hw_avg.info()\n",
    "df_no_hw_avg_reset = df_no_hw_avg.reset_index()\n",
    "df_no_hw_avg_reset.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fba9ed8b47547a8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##   Step 2.2: Merge df_hw with df_no_hw_avg_reset"
   ]
  },
  {
   "cell_type": "code",
   "id": "1235cd40864887ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "merged_df = pd.merge(df_hw_reset, df_no_hw_avg_reset[['lat', 'lon', 'year', 'hour', 'UHI', 'UBWI']],\n",
    "                     on=['lat', 'lon', 'year', 'hour'],\n",
    "                     suffixes=('', '_avg'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b425e5780987917",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "merged_df.info()\n",
    "merged_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a2ac989c16eea46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "##  Step 2.3: Subtract the average UHI from the hourly UHI and store in a new column"
   ]
  },
  {
   "cell_type": "code",
   "id": "44408a202722573",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "merged_df['UHI_diff'] = merged_df['UHI'] - merged_df['UHI_avg']\n",
    "merged_df['UBWI_diff'] = merged_df['UBWI'] - merged_df['UBWI_avg']\n",
    "# Now, merged_df contains your original data along with the subtracted UHI values in 'UHI_diff'\n",
    "merged_df  # To check the first few rows of the merged DataFrame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1463f2a5168f843e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "merged_df[['UHI_diff', 'UBWI_diff']].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f8948bf",
   "metadata": {},
   "source": [
    "merged_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c99696a2c4ea4ab9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## (Optional) Step 2.4: Validate the results by checking the UHI values for a specific location and time"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e223411d549cb4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "row_index = 198\n",
    "print(merged_df.iloc[row_index].UHI_avg)\n",
    "a_row = merged_df.iloc[row_index]\n",
    "df_no_hw_avg_reset.loc[(df_no_hw_avg_reset['lat'] == a_row.lat) & (df_no_hw_avg_reset['lon'] == a_row.lon) & (\n",
    "                df_no_hw_avg_reset['year'] == a_row.year) & (\n",
    "                df_no_hw_avg_reset['hour'] == a_row.hour)].UHI\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0fdb0d3",
   "metadata": {},
   "source": [
    "#  3: Averaged data for each local hour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9969b76",
   "metadata": {},
   "source": [
    "##  Step 3.1: Adjust to local hour"
   ]
  },
  {
   "cell_type": "code",
   "id": "5701f094",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_time_to_local_and_add_hour(df):\n",
    "    \"\"\"\n",
    "    Converts the UTC timestamp in the DataFrame to local time based on longitude and adds a column for the local hour.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns for latitude ('lat'), longitude ('lon'), and UTC timestamp ('time')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns ('local_time' and 'local_hour') for the timestamp adjusted to local time and the hour extracted from the local time\n",
    "    \"\"\"\n",
    "    # Function to calculate timezone offset from longitude\n",
    "    def calculate_timezone_offset(longitude):\n",
    "        return np.floor(longitude / 15.0).astype(int)  # Approximate, not accounting for DST or specific timezone rules\n",
    "\n",
    "    # Calculate timezone offsets for each row based on longitude\n",
    "    offsets = calculate_timezone_offset(df['lon'].values)\n",
    "\n",
    "    # Adjust timestamps by the offsets\n",
    "    df['local_time'] = df['time'] + pd.to_timedelta(offsets, unit='h')\n",
    "\n",
    "    # Extract the hour from the 'local_time' and create a new column\n",
    "    df['local_hour'] = df['local_time'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Assuming 'df' is your original DataFrame\n",
    "# # Make sure 'time' column is in datetime format\n",
    "# df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# # Convert UTC times to local times based on longitude and add local hour\n",
    "# df = convert_time_to_local_and_add_hour(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4a90d34",
   "metadata": {},
   "source": [
    "merged_df = convert_time_to_local_and_add_hour(merged_df)\n",
    "merged_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a5ff8a1",
   "metadata": {},
   "source": [
    "merged_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49ea995b",
   "metadata": {},
   "source": [
    "##  Step 3.2 compute average based on local hour"
   ]
  },
  {
   "cell_type": "code",
   "id": "447d0ccd",
   "metadata": {},
   "source": [
    "os.environ[\"PROJ_LIB\"] = \"/home/jguo/anaconda3/envs/I2000/share/proj\"\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Group by 'lat', 'lon', and 'local_hour', then calculate the mean for 'UHI_diff'\n",
    "# Ensure grouped data is sorted by 'lat' and 'lon' before pivoting\n",
    "var_diff_by_localhour = merged_df.groupby(['lat', 'lon', 'local_hour'])[['UHI_diff', 'UBWI_diff']].mean().reset_index().sort_values(by=['lat', 'lon', 'local_hour'])\n",
    "\n",
    "var_diff_by_localhour\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "var_diff_by_localhour.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3cfbaf16ab84993",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29f22336",
   "metadata": {},
   "source": [
    "# 4: Plot and Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## See the hourly pattern of UHI_diff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63cf1bcaf6b55686"
  },
  {
   "cell_type": "code",
   "id": "2b83c80de8d0e6fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function to normalize longitude values to the range [-180, 180]\n",
    "def normalize_longitude(lon):\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'var_diff_by_localhour' is your DataFrame\n",
    "\n",
    "# Function to normalize longitude values to the range [-180, 180]\n",
    "def normalize_longitude(lon):\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "# Normalize the longitude values in your DataFrame\n",
    "var_diff_by_localhour['lon'] = var_diff_by_localhour['lon'].apply(normalize_longitude)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "diff_variabel_name = 'UBWI_diff'\n",
    "\n",
    "\n",
    "# Define the map drawing function for subplots\n",
    "def draw_map_subplot(m, ax):\n",
    "    m.drawcoastlines(linewidth=0.5, ax=ax)\n",
    "    m.drawcountries(linewidth=0.5, ax=ax)\n",
    "    m.fillcontinents(color='coral', lake_color='aqua', alpha=0.3, ax=ax)\n",
    "    m.drawmapboundary(fill_color='aqua', ax=ax)\n",
    "    m.drawparallels(np.arange(-90., 91., 30.), labels=[1, 0, 0, 0], fontsize=10, ax=ax)\n",
    "    m.drawmeridians(np.arange(-180., 181., 60.), labels=[0, 0, 0, 1], fontsize=10, ax=ax)\n",
    "\n",
    "# Find global min and max of UHI_diff\n",
    "global_min = var_diff_by_localhour[diff_variabel_name].min()\n",
    "global_max = var_diff_by_localhour[diff_variabel_name].max()\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "n_hours = len(var_diff_by_localhour['local_hour'].unique())\n",
    "n_rows = (n_hours + 2) // 3  # Adding 2 to ensure rounding up if there's a remainder\n",
    "\n",
    "# Create a figure to hold all subplots\n",
    "fig, axs = plt.subplots(n_rows, 3, figsize=(18, n_rows * 6), constrained_layout=True)\n",
    "\n",
    "# Iterate through each local_hour to create subplots\n",
    "for i, hour in enumerate(var_diff_by_localhour['local_hour'].unique()):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = axs[row, col] if n_rows > 1 else axs[col]  # Adjust for the case of a single row\n",
    "\n",
    "    # Setup the Basemap\n",
    "    m = Basemap(projection='cyl', resolution='l', lat_0=0, lon_0=0, ax=ax)\n",
    "\n",
    "    draw_map_subplot(m, ax)\n",
    "\n",
    "    # Filter data for the current hour\n",
    "    df_hour = var_diff_by_localhour[var_diff_by_localhour['local_hour'] == hour]\n",
    "\n",
    "    # Scatter UHI_diff data\n",
    "    x, y = m(df_hour['lon'].values, df_hour['lat'].values)\n",
    "    # Set vmin and vmax to the global min/max values\n",
    "    sc = m.scatter(x, y, c=df_hour[diff_variabel_name], vmin=global_min, vmax=global_max, cmap='hot', marker='o', edgecolor='none', alpha=0.75, ax=ax)\n",
    "\n",
    "    ax.set_title(f'UHI Difference Map at Local Hour {hour}')\n",
    "\n",
    "    # Add color bar to each subplot\n",
    "    plt.colorbar(sc, ax=ax, fraction=0.046, pad=0.04, label='UHI_diff')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {},
   "id": "bc9c690b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UHI_diff by local hour"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93877d3d0579b738"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a53e34490328c030",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Grouping by 'local_hour' and calculating the mean of 'UHI_diff'\n",
    "uhi_hourly = var_diff_by_localhour.groupby('local_hour')['UBWI_diff'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(uhi_hourly['local_hour'], uhi_hourly['UBWI_diff'], marker='o')\n",
    "plt.title('Mean UHI Difference by Local Hour')\n",
    "plt.xlabel('Local Hour')\n",
    "plt.ylabel('Mean UHI Difference')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da11d2af0994ccc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have some understanding about UHI:\n",
    "1. Diurnal Cycle of Solar Radiation: During the daytime, solar radiation is the primary source of heat for both urban and rural areas. However, urban surfaces, with their lower albedo and higher heat capacity, tend to absorb and store more solar radiation than rural surfaces. This leads to a smaller UHI effect during the day. At night, the absence of solar radiation allows the stored heat in urban areas to be gradually released, resulting in a stronger UHI effect at night.\n",
    "2. Boundary Layer Dynamics: The atmospheric boundary layer, the layer of air closest to the Earth's surface, plays a crucial role in heat dissipation. During the day, the boundary layer is typically deeper and more turbulent, promoting greater mixing and heat exchange between the surface and the atmosphere. This can help to mitigate the UHI effect. At night, the boundary layer becomes shallower and more stable, trapping heat near the surface and intensifying the UHI effect.\n",
    "\n",
    "I need to better understand why the synergy is also showing a diurnal cycle."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7e731943da98485"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Koppen Geiger Climate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f52ed687cc6ac5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  step 1: load the koppen geiger map and legend"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a66ce85491ac5b"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the NetCDF file\n",
    "ds_koppen_map = xr.open_dataset('/home/jguo/other_projects/1991_2020/koppen_geiger_0p5.nc')\n",
    "#ds_koppen_map.kg_class.min()\n",
    "\n",
    "# Load the Koppen Geiger Legend Excel file\n",
    "kg_legend = pd.read_excel('/home/jguo/research/hw_global/Data/KoppenGeigerLegend.xlsx', engine='openpyxl')\n",
    "kg_legend"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a04f06fb017e15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  step 2: Find the nearest Koppen Geiger class for each grid cell"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dcc00479705bdd4"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Convert latitudes and longitudes from the NetCDF dataset to numpy arrays\n",
    "latitudes = ds_koppen_map['lat'].values\n",
    "longitudes = ds_koppen_map['lon'].values\n",
    "\n",
    "# Flatten the latitudes, longitudes, and kg_class for easier manipulation\n",
    "lat_flat = np.repeat(latitudes, len(longitudes))\n",
    "lon_flat = np.tile(longitudes, len(latitudes))\n",
    "kg_class_flat = ds_koppen_map['kg_class'].values.flatten()\n",
    "\n",
    "# Filter out the zero kg_class values\n",
    "non_zero_indices = kg_class_flat > 0\n",
    "lat_flat_non_zero = lat_flat[non_zero_indices]\n",
    "lon_flat_non_zero = lon_flat[non_zero_indices]\n",
    "kg_class_flat_non_zero = kg_class_flat[non_zero_indices]\n",
    "\n",
    "# Function to find the nearest non-zero kg_class for given lat and lon\n",
    "def find_nearest_non_zero_kg_class(lat, lon):\n",
    "    distances = np.sqrt((lat_flat_non_zero - lat)**2 + (lon_flat_non_zero - lon)**2)\n",
    "    nearest_index = np.argmin(distances)\n",
    "    return kg_class_flat_non_zero[nearest_index]\n",
    "\n",
    "# Vectorize the function to apply it efficiently to arrays\n",
    "vec_find_nearest_non_zero_kg_class = np.vectorize(find_nearest_non_zero_kg_class)\n",
    "\n",
    "# Apply the vectorized function to each lat and lon in var_diff_by_localhour\n",
    "var_diff_by_localhour['KG_ID'] = vec_find_nearest_non_zero_kg_class(var_diff_by_localhour['lat'].values, var_diff_by_localhour['lon'].values)\n",
    "var_diff_by_localhour"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37f7b7ec6dbbc6d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  step 3: Plot the UHI_diff by local hour for each Koppen Geiger class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cb68449c9414c"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Calculate average UHI_diff by local_hour for each KG class\n",
    "avg_uhi_by_hour_and_kg = var_diff_by_localhour.groupby(['KG_ID', 'local_hour'])[diff_variabel_name].mean().reset_index()\n",
    "# Map KG classes to their descriptions\n",
    "kg_map = dict(zip(kg_legend['ID'], kg_legend['KGClass']))\n",
    "# Plotting\n",
    "import textwrap\n",
    "\n",
    "# Define the number of graphs you want in each row\n",
    "graphs_per_row = 4  # You can change this number to your preference\n",
    "\n",
    "# Find the global minimum and maximum UHI_diff values for consistent y-axis limits\n",
    "global_min_uhi = avg_uhi_by_hour_and_kg[diff_variabel_name].min()\n",
    "global_max_uhi = avg_uhi_by_hour_and_kg[diff_variabel_name].max()\n",
    "\n",
    "# Unique KG IDs\n",
    "unique_kg_ids = avg_uhi_by_hour_and_kg['KG_ID'].unique()\n",
    "\n",
    "# Number of KG IDs\n",
    "n_kg_ids = len(unique_kg_ids)\n",
    "\n",
    "# Calculate the number of rows needed\n",
    "n_rows = (n_kg_ids + graphs_per_row - 1) // graphs_per_row  # Ensures rounding up\n",
    "\n",
    "# Loop through each KG ID\n",
    "for i, kg_id in enumerate(unique_kg_ids):\n",
    "    # Create a new figure at the start and after every 'graphs_per_row' plots\n",
    "    if i % graphs_per_row == 0:\n",
    "        plt.figure(figsize=(5 * graphs_per_row, 5 * n_rows))  # Adjust figure size as needed\n",
    "    # Select the subplot position\n",
    "    plt.subplot(n_rows, graphs_per_row, i % graphs_per_row + 1)\n",
    "\n",
    "    # Extract the subset of data for the current KG ID\n",
    "    subset = avg_uhi_by_hour_and_kg[avg_uhi_by_hour_and_kg['KG_ID'] == kg_id]\n",
    "\n",
    "    # Plot the data\n",
    "    plt.plot(subset['local_hour'], subset[diff_variabel_name], marker='o')\n",
    "\n",
    "    # Wrap the title text\n",
    "    title_text = f'KG Class {kg_id}: {kg_map.get(kg_id, \"Unknown\")} - Average Hourly UHI_diff'\n",
    "    wrapped_title = textwrap.fill(title_text, width=40)  # Adjust 'width' as needed\n",
    "\n",
    "    plt.title(wrapped_title)\n",
    "    plt.xlabel('Local Hour')\n",
    "    plt.ylabel('Average UBWI_diff')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Set the same y-axis limits for all plots\n",
    "    plt.ylim(global_min_uhi, global_max_uhi)\n",
    "\n",
    "    # Show the figure after every 'graphs_per_row' plots or on the last plot\n",
    "    if (i % graphs_per_row == graphs_per_row - 1) or (i == n_kg_ids - 1):\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69c093ac73df470f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Next Steps: Statistical Analysis and Interpretation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4daa1570cc9e8d5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1: Statistical Analysis: LHS UHI_diff or UBWI_diff. RHS KG_ID (major and sub categories), day and night (we are going to collapse the hours into day and night)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "241b381f2b80494f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2: Try using UHI_hour on the LHS and perform similar analysis as above.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be695cf334c65b05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3: Coastline analysis. (optional because Kopphen Geiger zones has in a way already capture the coastal effect to me) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3593e02695990497"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#   Miscalanous items"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8590a7c15580328f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count number of HW days"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be5d5b200169e969"
  },
  {
   "cell_type": "code",
   "id": "57cf239f91ba7eef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_count = df_hw.groupby(['lat', 'lon']).size()\n",
    "df_count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_count_sorted = df_count.sort_values(ascending=False).reset_index()\n",
    "df_count_sorted"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb71ff1d912538f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df_count_sorted['lon'] = df_count_sorted['lon'].apply(normalize_longitude)\n",
    "df_count_sorted = df_count_sorted.set_index(['lat', 'lon']) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6f36a74b8e8eac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af55ac33c9e875f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_count_sorted /= 365 # number of HW days per year\n",
    "\n",
    "# Create a Basemap instance for a simple cylindrical projection, with coastlines and country boundaries\n",
    "m = Basemap(projection='cyl', resolution='c', llcrnrlat=-90, urcrnrlat=90, llcrnrlon=-180, urcrnrlon=180)\n",
    "m.drawcoastlines(linewidth=0.5)\n",
    "m.drawcountries(linewidth=0.5)\n",
    "\n",
    "# Convert latitudes and longitudes to map projection coordinates\n",
    "x, y = m(df_count_sorted.index.get_level_values('lon').values, df_count_sorted.index.get_level_values('lat').values)\n",
    "\n",
    "# Scatter plot: plotting the total HW values for each grid cell on the map\n",
    "m.scatter(x, y, c=df_count_sorted.values, s=10, cmap='Reds', alpha=0.75, latlon=False)\n",
    "\n",
    "# Add a color bar to indicate the scale of HW values\n",
    "plt.colorbar(label='Total HW / Year')\n",
    "\n",
    "# Title for the map\n",
    "plt.title('Global Map Showing Total HW on Each Grid Cell')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## group by north south hemisphere"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2bfe88e76bdfdb"
  },
  {
   "cell_type": "code",
   "source": [
    "var_diff_by_localhour.groupby(lambda x: (var_diff_by_localhour.loc[x, 'lat'] > 0, var_diff_by_localhour.loc[x, 'local_hour']))['UHI_diff'].mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5adcb30c2eb21982",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1385d765c260ff92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load koppen map tiff file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a182e4909df8215"
  },
  {
   "cell_type": "code",
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# Open the GeoTIFF file as an xarray DataSet\n",
    "#file_path = '/home/jguo/other_projects/koppen_geiger_tif/1991_2020/koppen_geiger_0p1.tif'\n",
    "pic_path = '/home/jguo/other_projects/koppen_geiger_tif/1991_2020/koppen_geiger_1p0.tif'\n",
    "# xds = rioxarray.open_rasterio('/home/jguo/other_projects/koppen_geiger_tif/1991_2020/koppen_geiger_0p1.tif')\n",
    "ds_koppen = xr.open_dataset(pic_path, engine=\"rasterio\")\n",
    "# Now `xds` is an xarray DataSet with the raster data and spatial coordinates\n",
    "ds_koppen\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be00840910df0bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import rasterio\n",
    "src = rasterio.open(pic_path)\n",
    "src.colormap(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e858685f843bb8c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ds_koppen.band_data.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "612aaf125e4f286d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ds_koppen.to_dataframe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee6a7808e865502e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
