{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T20:16:37.451899Z",
     "start_time": "2024-04-19T20:16:37.445614Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Step 1: Load the data",
   "id": "c945989e28871ab9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:16:39.468750Z",
     "start_time": "2024-04-19T20:16:37.513499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary_dir = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/summary'\n",
    "\n",
    "# merged_feather_path = os.path.join(summary_dir, 'local_hour_adjusted_variables.feather')\n",
    "merged_feather_path = os.path.join(summary_dir, 'local_hour_adjusted_variables_with_location_ID.feather')\n",
    "\n",
    "local_hour_adjusted_df = pd.read_feather(merged_feather_path)\n",
    "local_hour_adjusted_df.info()"
   ],
   "id": "571b7b782ae373fc",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Step 2: Create event ID",
   "id": "e330b0e4ce2da56f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:18:00.366224Z",
     "start_time": "2024-04-19T20:16:39.470662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort by 'location_ID' and 'time'\n",
    "local_hour_adjusted_df.sort_values(by=['location_ID', 'time'], inplace=True)\n",
    "\n",
    "# Create a new column 'time_diff' to find the difference in hours between consecutive rows\n",
    "local_hour_adjusted_df['time_diff'] = local_hour_adjusted_df.groupby('location_ID')['time'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "# Identify the start of a new event (any gap of more than one hour)\n",
    "local_hour_adjusted_df['new_event'] = (local_hour_adjusted_df['time_diff'] > 1)\n",
    "\n",
    "\n",
    "# Generate cumulative sum to assign unique event IDs within each location\n",
    "local_hour_adjusted_df['event_ID'] = local_hour_adjusted_df.groupby('location_ID')['new_event'].cumsum()\n",
    "\n",
    "#Combine location_ID with event_ID to create a globally unique event identifier\n",
    "local_hour_adjusted_df['global_event_ID'] = local_hour_adjusted_df['location_ID'].astype(str) + '_' + local_hour_adjusted_df['event_ID'].astype(str)\n",
    "\n",
    "# # Drop the helper columns if they are no longer needed\n",
    "# local_hour_adjusted_df.drop(columns=['time_diff', 'new_event'], inplace=True)\n",
    "\n",
    "# Now the DataFrame 'local_hour_adjusted_df' includes a unique 'global_event_ID' for each heatwave event\n"
   ],
   "id": "e93f2bafef473aba",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:18:00.408238Z",
     "start_time": "2024-04-19T20:18:00.368783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_hour_adjusted_df.info()\n",
    "local_hour_adjusted_df.head(200)"
   ],
   "id": "f090b196596ce6cc",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 2.2 Check the continuity of dates within each event",
   "id": "9513d86d28dec5dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:21.059311Z",
     "start_time": "2024-04-19T20:18:00.410024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to validate continuity of events for each location\n",
    "def validate_event_continuity(df):\n",
    "    # Group by location_ID and event_ID\n",
    "    grouped = df.groupby(['location_ID', 'event_ID'])\n",
    "    errors = []  # To store any errors found during validation\n",
    "\n",
    "    # Iterate through each group\n",
    "    for (location_id, event_id), group in grouped:\n",
    "        # Sort timestamps to ensure sequential processing\n",
    "        sorted_times = group['time'].sort_values().tolist()\n",
    "\n",
    "        # Check if each timestamp is no more than an hour apart from the next\n",
    "        for i in range(1, len(sorted_times)):\n",
    "            if (sorted_times[i] - sorted_times[i - 1]).total_seconds() > 3600:\n",
    "                errors.append(f\"Gap of over an hour found in event {event_id} for location {location_id}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "# Validate event continuity\n",
    "continuity_errors = validate_event_continuity(local_hour_adjusted_df)\n",
    "if continuity_errors:\n",
    "    print(\"Continuity Errors:\")\n",
    "    for error in continuity_errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All events are continuous with no gaps of over an hour.\")\n"
   ],
   "id": "28f20bf48a02ba12",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 2.3 Check the uniqueness of event IDs within each location:",
   "id": "d2eade15ce317cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:29.197053Z",
     "start_time": "2024-04-19T20:21:21.061172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if event IDs are unique across all locations and continuous heatwave periods\n",
    "is_unique = local_hour_adjusted_df['global_event_ID'].nunique() == local_hour_adjusted_df.groupby(['location_ID', 'global_event_ID']).ngroups\n",
    "print(\"Event IDs are unique across all locations and continuous heatwave periods:\", is_unique)\n"
   ],
   "id": "4774ec0601b395d8",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2.4 Manually inspect a few events",
   "id": "4c96fa8aefddbbe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:34.280012Z",
     "start_time": "2024-04-19T20:21:29.198715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect a few events manually\n",
    "sample_events = local_hour_adjusted_df.groupby(['location_ID', 'event_ID']).head(1).sort_values('event_ID')\n",
    "print(sample_events[['location_ID', 'event_ID', 'local_time']])"
   ],
   "id": "ff27d75792b522e5",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:34.284588Z",
     "start_time": "2024-04-19T20:21:34.281740Z"
    }
   },
   "cell_type": "code",
   "source": "# sample_events[['location_ID', 'event_ID', 'local_time']]",
   "id": "4c3be6aedadba4c3",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:34.311166Z",
     "start_time": "2024-04-19T20:21:34.285430Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df.head()",
   "id": "f8400673f02d3ae8",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2.5 Save the updated DataFrame",
   "id": "bc5dae43fa322cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:47:03.646226Z",
     "start_time": "2024-04-19T20:47:03.635077Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df.info()",
   "id": "889bdc6e37926cf",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:18.575649Z",
     "start_time": "2024-04-19T20:48:06.593766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the updated DataFrame with event IDs\n",
    "merged_feather_path = os.path.join(summary_dir, 'local_hour_with_location_id_event_id.feather')\n",
    "# Reset the index to convert it into a column\n",
    "local_hour_adjusted_df_reset = local_hour_adjusted_df.reset_index()\n",
    "\n",
    "# Now save to Feather\n",
    "local_hour_adjusted_df_reset.to_feather(merged_feather_path)\n"
   ],
   "id": "a0f7ae112a9d1b7b",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:33.332194Z",
     "start_time": "2024-04-19T20:48:33.321199Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df_reset.info()",
   "id": "4de5a8bef83bb289",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# step 3: For each urban grid, identify HWs with positive and negative UHI-HW interactions and then calculate the mean UHI_diff value. Then compare the meteorological conditions (air temperature, humidity, wind, planet boundary layer depth, etc.) between the positive UHI-HW-interaction event and negative UHI-HW-interaction event. ",
   "id": "19a3e1950fd63648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Question\n",
    "How do I find planet boundary layer depth?"
   ],
   "id": "c6ae358245d687dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:21:34.314515Z",
     "start_time": "2024-04-19T20:21:34.312609Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c3283f1f8a09b1d8",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Step 3.1: define day and night time \n",
    "Daytime: 08:00 to 16:00 local time. (Keer paper)\n",
    "Nighttime: 20:00 to 04:00 local time."
   ],
   "id": "19e78db8f7730494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:50:47.017204Z",
     "start_time": "2024-04-19T20:50:46.601882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'local_hour_adjusted_df' is your DataFrame name\n",
    "\n",
    "# Step 1: Define masks for daytime and nighttime\n",
    "daytime_mask = local_hour_adjusted_df['local_hour'].between(8, 16)\n",
    "nighttime_mask = (local_hour_adjusted_df['local_hour'].between(20, 24) |\n",
    "                  local_hour_adjusted_df['local_hour'].between(0, 4))\n",
    "\n",
    "\n"
   ],
   "id": "cb66e99d611684d5",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 3.2: Calculate the mean UHI_diff value for each event day and night",
   "id": "1a484abbda5b2ed9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:52:48.088921Z",
     "start_time": "2024-04-19T20:52:30.565399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to compute averages for UHI_diff based on given mask\n",
    "def compute_uhi_diff_averages(df, mask):\n",
    "    return df[mask].groupby('global_event_ID')['UHI_diff'].mean()\n",
    "\n",
    "# Calculate averages for UHI_diff for daytime and nighttime\n",
    "daytime_uhi_diff_avg = compute_uhi_diff_averages(local_hour_adjusted_df, daytime_mask)\n",
    "nighttime_uhi_diff_avg = compute_uhi_diff_averages(local_hour_adjusted_df, nighttime_mask)"
   ],
   "id": "3519a8445aeb2313",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:29:46.836531Z",
     "start_time": "2024-04-19T21:29:39.704115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Compute simple averages for all other relevant columns\n",
    "columns_to_average = [ 'UHI_diff', 'UHI', 'UWBI', 'WIND', 'RAIN', 'SNOW', \n",
    "                      'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']\n",
    "uhi_diff_avg_df = local_hour_adjusted_df.groupby('global_event_ID')[columns_to_average].mean()\n",
    "uhi_diff_avg_df.info()"
   ],
   "id": "17a55281dda41475",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:30:01.324876Z",
     "start_time": "2024-04-19T21:30:01.227808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Add daytime and nighttime UHI_diff averages to the dataframe\n",
    "uhi_diff_avg_df['UHI_diff_daytime'] = daytime_uhi_diff_avg\n",
    "uhi_diff_avg_df['UHI_diff_nighttime'] = nighttime_uhi_diff_avg\n",
    "\n",
    "# The resulting DataFrame, 'simple_averages_df', now includes the requested columns"
   ],
   "id": "c842e3bc28aec06d",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:30:15.731503Z",
     "start_time": "2024-04-19T21:30:15.707082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uhi_diff_avg_df.info()\n",
    "uhi_diff_avg_df.head(300)\n"
   ],
   "id": "7f6c2952d0e1329a",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:30:55.926209Z",
     "start_time": "2024-04-19T21:30:55.853520Z"
    }
   },
   "cell_type": "code",
   "source": "uhi_diff_avg_df.query('UHI_diff< 0').count()",
   "id": "a715bc6b1f32ed0e",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Data Analysis\n",
   "id": "85f35b2ca3e59534"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-19T21:21:20.944283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "# Assuming 'uhi_diff_avg_df' is already loaded in your environment\n",
    "\n",
    "# Separate the data into two groups\n",
    "negative_uhi_diff = uhi_diff_avg_df[uhi_diff_avg_df['UHI_diff'] < 0]\n",
    "positive_uhi_diff = uhi_diff_avg_df[uhi_diff_avg_df['UHI_diff'] > 0]\n",
    "\n",
    "# Define non-UHI columns\n",
    "non_uhi_columns = ['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']\n",
    "\n",
    "# # Descriptive Statistics\n",
    "# print(\"Descriptive Statistics for UHI_diff < 0:\")\n",
    "# print(negative_uhi_diff[non_uhi_columns].describe())\n",
    "# print(\"\\nDescriptive Statistics for UHI_diff > 0:\")\n",
    "# print(positive_uhi_diff[non_uhi_columns].describe())"
   ],
   "id": "c827bc5d8ee8d858",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:31:35.791556Z",
     "start_time": "2024-04-19T21:31:35.773752Z"
    }
   },
   "cell_type": "code",
   "source": "negative_uhi_diff",
   "id": "6cc28a04052c50fc",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:31:57.018932Z",
     "start_time": "2024-04-19T21:31:55.748858Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df.query('global_event_ID == \"15782_1\"')",
   "id": "863ab8deffda23d8",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:44:47.709081Z",
     "start_time": "2024-04-19T21:44:47.617443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Correlation Analysis\n",
    "correlations = uhi_diff_avg_df[non_uhi_columns + ['UHI_diff']].corr()['UHI_diff']\n",
    "print(\"\\nCorrelations with UHI_diff:\")\n",
    "print(correlations)\n"
   ],
   "id": "a571a518e071fe46",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Which Variable is contributing to the UHI_diff",
   "id": "206b4440e8b3fa7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 5.1: Using logistic regression",
   "id": "f1a6191335b5828f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:38:03.856773Z",
     "start_time": "2024-04-19T22:23:35.836160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'uhi_diff_avg_df' is already in your environment\n",
    "\n",
    "# Prepare the data\n",
    "X = uhi_diff_avg_df[['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']]\n",
    "y = (uhi_diff_avg_df['UHI_diff'] > 0).astype(int)  # Create a binary target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Summarize the background data using shap.kmeans\n",
    "background_data = shap.kmeans(X_train_scaled, k=30)  # Summarize with 30 representative clusters\n",
    "\n",
    "# Create SHAP values using KernelExplainer with the summarized background\n",
    "explainer = shap.KernelExplainer(model.predict_proba, background_data, link=\"logit\", n_jobs = 32)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Plot the SHAP values for the positive class\n",
    "shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X.columns, plot_type=\"bar\")\n",
    "\n",
    "\n"
   ],
   "id": "b0b3f1a4b096bca",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:49:33.709097Z",
     "start_time": "2024-04-19T21:49:31.130952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'uhi_diff_avg_df' is already in your environment\n",
    "\n",
    "# Prepare the data\n",
    "X = uhi_diff_avg_df[['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']]\n",
    "y = uhi_diff_avg_df['UHI']  # Assuming you want to predict UHI_diff directly\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create SHAP values\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summarize the SHAP values in a plot to show the impact of each feature\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ],
   "id": "146cca73d087ff61",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "strong positive correlation UHI and HW\n",
    "negative event \n",
    "insignficant event \n",
    "global map \n",
    "for each grid # postive and negative interaction \n"
   ],
   "id": "1361be684a2f3d35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
