{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import netCDF4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import mlflow\n",
    "import mlflow.catboost\n",
    "import mlflow.shap\n",
    "from scipy.stats import linregress\n",
    "import argparse\n",
    "\n",
    "# # Parse arguments\n",
    "# parser = argparse.ArgumentParser(description=\"Run UHI model for day or night data.\")\n",
    "# parser.add_argument(\"--time_period\", choices=[\"day\", \"night\"], required=True, help=\"Specify whether to run for day or night data.\")\n",
    "# args = parser.parse_args()\n",
    "description=\"Run UHI model for day or night data.\"\n",
    "time_period = \"day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set summary directory and experiment name\n",
    "summary_dir = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/summary'\n",
    "experiment_name = f'nb_1985_UHI_{time_period.capitalize()}_add_delta_FSA'\n",
    "\n",
    "# Create the MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.start_run()\n",
    "\n",
    "# Create directory for saving figures and artifacts\n",
    "figure_dir = os.path.join(summary_dir, 'mlflow', experiment_name)\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "merged_feather_path = os.path.join(summary_dir, 'local_hour_adjusted_variables_with_location_ID_event_ID_and_sur.feather')\n",
    "local_hour_adjusted_df = pd.read_feather(merged_feather_path)\n",
    "\n",
    "# Filter data to have year 1985 only\n",
    "local_hour_adjusted_df = local_hour_adjusted_df[local_hour_adjusted_df['year'] == 1985]\n",
    "\n",
    "\n",
    "# Load location ID dataset\n",
    "location_ID_path = os.path.join(summary_dir, 'location_IDs.nc')\n",
    "location_ID_ds = xr.open_dataset(location_ID_path, engine='netcdf4')\n",
    "\n",
    "# Load feature list\n",
    "df_daily_vars = pd.read_excel('/home/jguo/research/hw_global/Data/hourlyDataSchema.xlsx')\n",
    "daily_vars = df_daily_vars.loc[df_daily_vars['X_vars2'] == 'Y', 'Variable']\n",
    "daily_var_lst = daily_vars.tolist()\n",
    "\n",
    "# Load delta feature list\n",
    "delta_vars = df_daily_vars.loc[df_daily_vars['X_vars_delta'] == 'Y', 'Variable']\n",
    "delta_var_lst = delta_vars.tolist()\n",
    "\n",
    "# Calculate delta variables and add to dataframe\n",
    "for var in delta_var_lst:\n",
    "    var_U = f\"{var}_U\"\n",
    "    var_R = f\"{var}_R\"\n",
    "    delta_var = f\"delta_{var}\"\n",
    "    if var_U in local_hour_adjusted_df.columns and var_R in local_hour_adjusted_df.columns:\n",
    "        local_hour_adjusted_df[delta_var] = local_hour_adjusted_df[var_U] - local_hour_adjusted_df[var_R]\n",
    "        daily_var_lst.append(delta_var)  # Add delta variable to daily_var_lst\n",
    "    else:\n",
    "        print(f\"Warning: {var_U} or {var_R} not found in dataframe columns.\")\n",
    "\n",
    "print(\"daily_var_lst\", daily_var_lst)\n",
    "\n",
    "# Save df_daily_vars to Excel file and log as artifact\n",
    "df_daily_vars_path = os.path.join(figure_dir, 'df_daily_vars.xlsx')\n",
    "df_daily_vars.to_excel(df_daily_vars_path, index=False)\n",
    "mlflow.log_artifact(df_daily_vars_path)\n",
    "\n",
    "# Save daily_var_lst to text file and log as artifact\n",
    "daily_var_lst_path = os.path.join(figure_dir, 'daily_var_lst.txt')\n",
    "with open(daily_var_lst_path, 'w') as f:\n",
    "    for var in daily_var_lst:\n",
    "        f.write(f\"{var}\\n\")\n",
    "mlflow.log_artifact(daily_var_lst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define helper functions\n",
    "def get_long_name(var_name, df_daily_vars):\n",
    "    if var_name.startswith('delta_'):\n",
    "        original_var_name = var_name.replace('delta_', '')\n",
    "        original_long_name = df_daily_vars.loc[df_daily_vars['Variable'] == original_var_name, 'Long Name'].values\n",
    "        if original_long_name.size > 0:\n",
    "            return f\"Difference of {original_long_name[0]}\"\n",
    "        else:\n",
    "            return f\"Difference of {original_var_name} (No long name found)\"\n",
    "    else:\n",
    "        long_name = df_daily_vars.loc[df_daily_vars['Variable'] == var_name, 'Long Name'].values\n",
    "        if long_name.size > 0:\n",
    "            return long_name[0]\n",
    "        else:\n",
    "            return f\"{var_name} (No long name found)\"\n",
    "\n",
    "def add_long_name(input_df, join_column='Feature', df_daily_vars=df_daily_vars):\n",
    "    input_df['Long Name'] = input_df[join_column].apply(lambda x: get_long_name(x, df_daily_vars))\n",
    "    return input_df\n",
    "\n",
    "# Define day and night masks\n",
    "daytime_mask = local_hour_adjusted_df['local_hour'].between(8, 16)\n",
    "nighttime_mask = (local_hour_adjusted_df['local_hour'].between(20, 24) | local_hour_adjusted_df['local_hour'].between(0, 4))\n",
    "\n",
    "# Separate daytime and nighttime data\n",
    "if time_period == \"day\":\n",
    "    uhi_diff = local_hour_adjusted_df[daytime_mask]\n",
    "else:\n",
    "    uhi_diff = local_hour_adjusted_df[nighttime_mask]\n",
    "\n",
    "X = uhi_diff[daily_var_lst]\n",
    "y = uhi_diff['UHI_diff']\n",
    "\n",
    "# Define linear slope function\n",
    "def feature_linear_slope(df, feature_name, label, confidence_level=0.95):\n",
    "    slope, _, _, p_value, stderr = linregress(df[feature_name], df[label])\n",
    "    t_value = np.abs(np.percentile(np.random.standard_t(df[feature_name].shape[0] - 2, 100000), [(1-confidence_level)/2, 1-(1-confidence_level)/2]))[1]\n",
    "    margin_of_error = t_value * stderr\n",
    "    return slope, margin_of_error, p_value\n",
    "\n",
    "def combine_slopes(daytime_df, nighttime_df, features, labels=['UHI', 'UHI_diff'], confidence_level=0.95):\n",
    "    data = {}\n",
    "    for feature in features:\n",
    "        slopes_with_intervals = []\n",
    "        for df, time in [(daytime_df, 'Day'), (nighttime_df, 'Night')]:\n",
    "            for label in labels:\n",
    "                slope, margin_of_error, p_value = feature_linear_slope(df, feature, label, confidence_level)\n",
    "                slope_with_interval = f\"{slope:.6f} (Â± {margin_of_error:.6f}, P: {p_value:.6f})\"\n",
    "                slopes_with_intervals.append(slope_with_interval)\n",
    "        data[feature] = slopes_with_intervals\n",
    "    columns = [f'{time}_{label}_slope' for time in ['Day', 'Night'] for label in labels]\n",
    "    results_df = pd.DataFrame(data, index=columns).transpose()\n",
    "    return results_df\n",
    "\n",
    "feature_names = daily_var_lst\n",
    "results_df = combine_slopes(local_hour_adjusted_df[daytime_mask], local_hour_adjusted_df[nighttime_mask], feature_names)\n",
    "sorted_results_df = results_df.sort_values('Day_UHI_slope', ascending=False)\n",
    "\n",
    "# Save and log the sorted results DataFrame\n",
    "sorted_results_path = os.path.join(figure_dir, 'sorted_results_df.csv')\n",
    "sorted_results_df.to_csv(sorted_results_path)\n",
    "mlflow.log_artifact(sorted_results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate(time_uhi_diff, daily_var_lst, model_name):\n",
    "    X = time_uhi_diff[daily_var_lst]\n",
    "    y = time_uhi_diff['UHI_diff']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    validation_pool = Pool(X_val, y_val)\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=6000,\n",
    "        learning_rate=0.03,\n",
    "        depth=7,\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        random_seed=42,\n",
    "        task_type='GPU',\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, \n",
    "              early_stopping_rounds=50, \n",
    "              plot=True, verbose=False)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_param(f\"{model_name}_iterations\", model.get_param('iterations'))\n",
    "    mlflow.log_param(f\"{model_name}_learning_rate\", model.get_param('learning_rate'))\n",
    "    mlflow.log_param(f\"{model_name}_depth\", model.get_param('depth'))\n",
    "\n",
    "        # Calculate and log metrics\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    val_rmse = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    val_r2 = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    mlflow.log_metric(f\"{model_name}_train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(f\"{model_name}_val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(f\"{model_name}_train_mae\", train_mae)\n",
    "    mlflow.log_metric(f\"{model_name}_val_mae\", val_mae)\n",
    "    mlflow.log_metric(f\"{model_name}_train_r2\", train_r2)\n",
    "    mlflow.log_metric(f\"{model_name}_val_r2\", val_r2)\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = train_and_evaluate(uhi_diff, daily_var_lst=daily_var_lst, model_name=f\"{time_period}_model\")\n",
    "\n",
    "# Log model\n",
    "mlflow.catboost.log_model(model, f\"{time_period}_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get feature importance\n",
    "def get_ordered_feature_importance(model: CatBoostRegressor, pool, type='FeatureImportance'):\n",
    "    if type == 'FeatureImportance':\n",
    "        feature_importances = model.get_feature_importance()\n",
    "    else:\n",
    "        feature_importances = model.get_feature_importance(pool, type=type)\n",
    "    \n",
    "    feature_names = pool.get_feature_names()\n",
    "    print(f\"Length of feature_importances: {len(feature_importances)}\")\n",
    "    print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "    \n",
    "    # Ensure the lengths match\n",
    "    if len(feature_importances) != len(feature_names):\n",
    "        raise ValueError(\"Feature importances and feature names lengths do not match\")\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    feature_importance_df = add_long_name(feature_importance_df, join_column='Feature')\n",
    "    return feature_importance_df\n",
    "\n",
    "full_pool = Pool(X, y)\n",
    "\n",
    "# Feature importance plots\n",
    "feature_importance = get_ordered_feature_importance(model, full_pool)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.title(f'{args.time_period.capitalize()}time Feature Importance')\n",
    "mlflow.log_figure(plt.gcf(), f'{args.time_period}time_feature_importance.png')\n",
    "plt.clf()\n",
    "\n",
    "# SHAP summary plots\n",
    "shap_values = model.get_feature_importance(full_pool, type='ShapValues')[:,:-1]\n",
    "shap.summary_plot(shap_values, X, show=False)\n",
    "plt.gcf().set_size_inches(15, 10)  # Adjust the figure size\n",
    "mlflow.log_figure(plt.gcf(), f'{args.time_period}_shap_summary_plot.png')\n",
    "plt.clf()\n",
    "\n",
    "# SHAP waterfall plots\n",
    "feature_importances = model.get_feature_importance()\n",
    "expected_value = shap_values[0, -1]\n",
    "long_names = [get_long_name(f, df_daily_vars) for f in full_pool.get_feature_names()]\n",
    "shap.waterfall_plot(shap.Explanation(feature_importances, base_values=expected_value, feature_names=long_names), show=False)\n",
    "plt.gcf().set_size_inches(15, 10)  # Adjust the figure size\n",
    "plt.gcf().subplots_adjust(left=0.3)  # Increase left margin to make room for y-axis labels\n",
    "mlflow.log_figure(plt.gcf(), f'{args.time_period}_shap_waterfall_plot.png')\n",
    "plt.clf()\n",
    "\n",
    "# SHAP dependence plots\n",
    "def plot_dependence_grid(shap_values, X, feature_names, time_period, target_feature='U10', plots_per_row=2):\n",
    "    feature_names = [f for f in feature_names if f != target_feature]\n",
    "    num_features = len(feature_names)\n",
    "    num_rows = (num_features + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(30, 10 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        shap.dependence_plot(ind=target_feature, shap_values=shap_values, features=X, interaction_index=feature_name, ax=axes[i],  show=False)\n",
    "        axes[i].set_title(f\"{time_period.capitalize()} time {target_feature} vs {feature_name}\")\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.set_size_inches(30, 10 * num_rows)\n",
    "    mlflow.log_figure(plt.gcf(), f'{time_period}_dependence_plot_{target_feature}.png')\n",
    "    plt.clf()\n",
    "\n",
    "top_features = feature_importance['Feature'].head(3).tolist()\n",
    "\n",
    "# Dependence plots\n",
    "for feature in top_features:\n",
    "    plot_dependence_grid(shap_values, X, feature_names=full_pool.get_feature_names(), time_period=args.time_period, target_feature=feature, plots_per_row=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.end_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipJupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
