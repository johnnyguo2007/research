{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:00.971778Z",
     "start_time": "2024-04-29T19:46:00.781306Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Step 1: Load the data",
   "id": "c945989e28871ab9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:08.551533Z",
     "start_time": "2024-04-29T19:46:00.973375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary_dir = '/Trex/test_case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/summary'\n",
    "\n",
    "# merged_feather_path = os.path.join(summary_dir, 'local_hour_adjusted_variables.feather')\n",
    "merged_feather_path = os.path.join(summary_dir, 'local_hour_adjusted_variables_with_location_ID_event_ID.feather')\n",
    "\n",
    "local_hour_adjusted_df = pd.read_feather(merged_feather_path)\n",
    "local_hour_adjusted_df.info()"
   ],
   "id": "571b7b782ae373fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58245960 entries, 8945088 to 56705927\n",
      "Data columns (total 73 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   index            int64         \n",
      " 1   time             datetime64[ns]\n",
      " 2   lat              float32       \n",
      " 3   lon              float32       \n",
      " 4   APPAR_TEMP       float32       \n",
      " 5   APPAR_TEMP_R     float32       \n",
      " 6   APPAR_TEMP_U     float32       \n",
      " 7   EFLX_LH_TOT      float32       \n",
      " 8   EFLX_LH_TOT_R    float32       \n",
      " 9   EFLX_LH_TOT_U    float32       \n",
      " 10  FGR              float32       \n",
      " 11  FGR_R            float32       \n",
      " 12  FGR_U            float32       \n",
      " 13  FIRA             float32       \n",
      " 14  FIRA_R           float32       \n",
      " 15  FIRA_U           float32       \n",
      " 16  FIRE             float32       \n",
      " 17  FIRE_R           float32       \n",
      " 18  FIRE_U           float32       \n",
      " 19  FLDS             float32       \n",
      " 20  FSA              float32       \n",
      " 21  FSA_R            float32       \n",
      " 22  FSA_U            float32       \n",
      " 23  FSDS             float32       \n",
      " 24  FSH              float32       \n",
      " 25  FSH_R            float32       \n",
      " 26  FSH_U            float32       \n",
      " 27  HEAT_FROM_AC     float32       \n",
      " 28  HIA              float32       \n",
      " 29  HIA_R            float32       \n",
      " 30  HIA_U            float32       \n",
      " 31  HW               float64       \n",
      " 32  PBOT             float32       \n",
      " 33  Q2M              float32       \n",
      " 34  Q2M_R            float32       \n",
      " 35  Q2M_U            float32       \n",
      " 36  QBOT             float32       \n",
      " 37  RAIN             float32       \n",
      " 38  SNOW             float32       \n",
      " 39  TBOT             float32       \n",
      " 40  THBOT            float32       \n",
      " 41  TSA              float32       \n",
      " 42  TSA_R            float32       \n",
      " 43  TSA_U            float32       \n",
      " 44  TSKIN            float32       \n",
      " 45  TSKIN_R          float32       \n",
      " 46  TSKIN_U          float32       \n",
      " 47  U10              float32       \n",
      " 48  UWBI             float32       \n",
      " 49  UHI              float32       \n",
      " 50  URBAN_HEAT       float32       \n",
      " 51  VAPOR_PRES       float32       \n",
      " 52  VAPOR_PRES_R     float32       \n",
      " 53  VAPOR_PRES_U     float32       \n",
      " 54  WASTEHEAT        float32       \n",
      " 55  WBA              float32       \n",
      " 56  WBA_R            float32       \n",
      " 57  WBA_U            float32       \n",
      " 58  WIND             float32       \n",
      " 59  hour             int32         \n",
      " 60  month            int32         \n",
      " 61  year             int32         \n",
      " 62  UHI_avg          float32       \n",
      " 63  UWBI_avg         float32       \n",
      " 64  UHI_diff         float32       \n",
      " 65  UWBI_diff        float32       \n",
      " 66  local_time       datetime64[ns]\n",
      " 67  local_hour       int32         \n",
      " 68  location_ID      int64         \n",
      " 69  time_diff        float64       \n",
      " 70  new_event        bool          \n",
      " 71  event_ID         int64         \n",
      " 72  global_event_ID  object        \n",
      "dtypes: bool(1), datetime64[ns](2), float32(60), float64(2), int32(4), int64(3), object(1)\n",
      "memory usage: 17.8+ GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Step 2: Validate the data",
   "id": "353bc674371bb21f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 2.1 Check the continuity of dates within each event",
   "id": "9513d86d28dec5dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:08.558680Z",
     "start_time": "2024-04-29T19:46:08.554022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Function to validate continuity of events for each location\n",
    "# def validate_event_continuity(df):\n",
    "#     # Group by location_ID and event_ID\n",
    "#     grouped = df.groupby(['location_ID', 'event_ID'])\n",
    "#     errors = []  # To store any errors found during validation\n",
    "# \n",
    "#     # Iterate through each group\n",
    "#     for (location_id, event_id), group in grouped:\n",
    "#         # Sort timestamps to ensure sequential processing\n",
    "#         sorted_times = group['time'].sort_values().tolist()\n",
    "# \n",
    "#         # Check if each timestamp is no more than an hour apart from the next\n",
    "#         for i in range(1, len(sorted_times)):\n",
    "#             if (sorted_times[i] - sorted_times[i - 1]).total_seconds() > 3600:\n",
    "#                 errors.append(f\"Gap of over an hour found in event {event_id} for location {location_id}\")\n",
    "# \n",
    "#     return errors\n",
    "# \n",
    "# # Validate event continuity\n",
    "# continuity_errors = validate_event_continuity(local_hour_adjusted_df)\n",
    "# if continuity_errors:\n",
    "#     print(\"Continuity Errors:\")\n",
    "#     for error in continuity_errors:\n",
    "#         print(error)\n",
    "# else:\n",
    "#     print(\"All events are continuous with no gaps of over an hour.\")\n"
   ],
   "id": "28f20bf48a02ba12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 2.2 Check the uniqueness of event IDs within each location:",
   "id": "d2eade15ce317cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:08.569422Z",
     "start_time": "2024-04-29T19:46:08.559958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Check if event IDs are unique across all locations and continuous heatwave periods\n",
    "# is_unique = local_hour_adjusted_df['global_event_ID'].nunique() == local_hour_adjusted_df.groupby(['location_ID', 'global_event_ID']).ngroups\n",
    "# print(\"Event IDs are unique across all locations and continuous heatwave periods:\", is_unique)\n"
   ],
   "id": "4774ec0601b395d8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2.3 Manually inspect a few events",
   "id": "4c96fa8aefddbbe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:14.021610Z",
     "start_time": "2024-04-29T19:46:08.570801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect a few events manually\n",
    "sample_events = local_hour_adjusted_df.groupby(['location_ID', 'event_ID']).head(1).sort_values('event_ID')\n",
    "print(sample_events[['location_ID', 'event_ID', 'local_time']])"
   ],
   "id": "ff27d75792b522e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          location_ID  event_ID          local_time\n",
      "234119          30757         0 1985-06-01 19:00:00\n",
      "911888          37245         0 1985-07-08 07:00:00\n",
      "15922723        30750         0 1993-06-01 18:00:00\n",
      "234118          30756         0 1985-06-01 19:00:00\n",
      "911889          37246         0 1985-07-08 07:00:00\n",
      "...               ...       ...                 ...\n",
      "56051199        21555       249 2013-02-26 20:00:00\n",
      "58092848        21555       250 2013-12-01 20:00:00\n",
      "58118932        21555       251 2013-12-06 20:00:00\n",
      "58152681        21555       252 2013-12-14 20:00:00\n",
      "58206302        21555       253 2013-12-24 20:00:00\n",
      "\n",
      "[293889 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:14.041300Z",
     "start_time": "2024-04-29T19:46:14.023084Z"
    }
   },
   "cell_type": "code",
   "source": "sample_events[['location_ID', 'event_ID', 'local_time', 'UHI_diff']].query('location_ID == 35793')",
   "id": "4c3be6aedadba4c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         location_ID  event_ID          local_time  UHI_diff\n",
       "4640102        35793         0 1987-06-01 06:00:00  0.347028"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_ID</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>local_time</th>\n",
       "      <th>UHI_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4640102</th>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 06:00:00</td>\n",
       "      <td>0.347028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:15.291274Z",
     "start_time": "2024-04-29T19:46:14.042401Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df[['lon', 'lat', 'location_ID', 'event_ID', 'local_time', 'UHI_diff']].query('location_ID == 35793')",
   "id": "4ee5c26f56d37a86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            lon        lat  location_ID  event_ID          local_time  \\\n",
       "4640102  101.25  26.858639        35793         0 1987-06-01 06:00:00   \n",
       "4641001  101.25  26.858639        35793         0 1987-06-01 07:00:00   \n",
       "4641900  101.25  26.858639        35793         0 1987-06-01 08:00:00   \n",
       "4642799  101.25  26.858639        35793         0 1987-06-01 09:00:00   \n",
       "4643698  101.25  26.858639        35793         0 1987-06-01 10:00:00   \n",
       "4644597  101.25  26.858639        35793         0 1987-06-01 11:00:00   \n",
       "4645496  101.25  26.858639        35793         0 1987-06-01 12:00:00   \n",
       "4646395  101.25  26.858639        35793         0 1987-06-01 13:00:00   \n",
       "4647294  101.25  26.858639        35793         0 1987-06-01 14:00:00   \n",
       "4648193  101.25  26.858639        35793         0 1987-06-01 15:00:00   \n",
       "4649092  101.25  26.858639        35793         0 1987-06-01 16:00:00   \n",
       "4649991  101.25  26.858639        35793         0 1987-06-01 17:00:00   \n",
       "4650890  101.25  26.858639        35793         0 1987-06-01 18:00:00   \n",
       "4651789  101.25  26.858639        35793         0 1987-06-01 19:00:00   \n",
       "4652688  101.25  26.858639        35793         0 1987-06-01 20:00:00   \n",
       "4653587  101.25  26.858639        35793         0 1987-06-01 21:00:00   \n",
       "4654486  101.25  26.858639        35793         0 1987-06-01 22:00:00   \n",
       "4655385  101.25  26.858639        35793         0 1987-06-01 23:00:00   \n",
       "4656284  101.25  26.858639        35793         0 1987-06-02 00:00:00   \n",
       "4657183  101.25  26.858639        35793         0 1987-06-02 01:00:00   \n",
       "4658082  101.25  26.858639        35793         0 1987-06-02 02:00:00   \n",
       "4658981  101.25  26.858639        35793         0 1987-06-02 03:00:00   \n",
       "4659880  101.25  26.858639        35793         0 1987-06-02 04:00:00   \n",
       "4660779  101.25  26.858639        35793         0 1987-06-02 05:00:00   \n",
       "\n",
       "         UHI_diff  \n",
       "4640102  0.347028  \n",
       "4641001 -0.118289  \n",
       "4641900 -0.255777  \n",
       "4642799  0.278869  \n",
       "4643698  0.474350  \n",
       "4644597  0.319997  \n",
       "4645496  0.251897  \n",
       "4646395  0.352888  \n",
       "4647294  0.285086  \n",
       "4648193  0.288741  \n",
       "4649092  0.212895  \n",
       "4649991  0.307031  \n",
       "4650890  0.422665  \n",
       "4651789  0.522335  \n",
       "4652688  0.490766  \n",
       "4653587  0.428886  \n",
       "4654486  0.366140  \n",
       "4655385  0.311915  \n",
       "4656284  0.224715  \n",
       "4657183  0.101098  \n",
       "4658082  0.018621  \n",
       "4658981  0.088403  \n",
       "4659880  0.239476  \n",
       "4660779  0.353669  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>location_ID</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>local_time</th>\n",
       "      <th>UHI_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4640102</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 06:00:00</td>\n",
       "      <td>0.347028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641001</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 07:00:00</td>\n",
       "      <td>-0.118289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641900</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 08:00:00</td>\n",
       "      <td>-0.255777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4642799</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 09:00:00</td>\n",
       "      <td>0.278869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643698</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 10:00:00</td>\n",
       "      <td>0.474350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644597</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 11:00:00</td>\n",
       "      <td>0.319997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645496</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 12:00:00</td>\n",
       "      <td>0.251897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646395</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 13:00:00</td>\n",
       "      <td>0.352888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647294</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 14:00:00</td>\n",
       "      <td>0.285086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648193</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 15:00:00</td>\n",
       "      <td>0.288741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649092</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 16:00:00</td>\n",
       "      <td>0.212895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649991</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 17:00:00</td>\n",
       "      <td>0.307031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650890</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 18:00:00</td>\n",
       "      <td>0.422665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4651789</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 19:00:00</td>\n",
       "      <td>0.522335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4652688</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 20:00:00</td>\n",
       "      <td>0.490766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4653587</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 21:00:00</td>\n",
       "      <td>0.428886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4654486</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 22:00:00</td>\n",
       "      <td>0.366140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4655385</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-01 23:00:00</td>\n",
       "      <td>0.311915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4656284</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 00:00:00</td>\n",
       "      <td>0.224715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4657183</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 01:00:00</td>\n",
       "      <td>0.101098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658082</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 02:00:00</td>\n",
       "      <td>0.018621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658981</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 03:00:00</td>\n",
       "      <td>0.088403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659880</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 04:00:00</td>\n",
       "      <td>0.239476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4660779</th>\n",
       "      <td>101.25</td>\n",
       "      <td>26.858639</td>\n",
       "      <td>35793</td>\n",
       "      <td>0</td>\n",
       "      <td>1987-06-02 05:00:00</td>\n",
       "      <td>0.353669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:15.307548Z",
     "start_time": "2024-04-29T19:46:15.292720Z"
    }
   },
   "cell_type": "code",
   "source": "local_hour_adjusted_df.head()",
   "id": "f8400673f02d3ae8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           index                time        lat    lon  APPAR_TEMP  \\\n",
       "8945088  8945088 1989-02-03 00:00:00 -45.706806  292.5   17.646959   \n",
       "8945250  8945250 1989-02-03 01:00:00 -45.706806  292.5   15.658759   \n",
       "8945412  8945412 1989-02-03 02:00:00 -45.706806  292.5   13.975966   \n",
       "8945574  8945574 1989-02-03 03:00:00 -45.706806  292.5   13.185209   \n",
       "8945736  8945736 1989-02-03 04:00:00 -45.706806  292.5   12.749510   \n",
       "\n",
       "         APPAR_TEMP_R  APPAR_TEMP_U  EFLX_LH_TOT  EFLX_LH_TOT_R  \\\n",
       "8945088     17.637291     18.878418    24.109003      24.298277   \n",
       "8945250     15.648606     16.951935     7.136189       7.192213   \n",
       "8945412     13.966308     15.206352     5.702584       5.747354   \n",
       "8945574     13.176082     14.347800     4.975313       5.014373   \n",
       "8945736     12.740907     13.845274     4.594939       4.631013   \n",
       "\n",
       "         EFLX_LH_TOT_U  ...  UWBI_avg  UHI_diff  UWBI_diff  \\\n",
       "8945088  -4.979587e-15  ... -0.031426  0.086491  -0.130616   \n",
       "8945250   0.000000e+00  ...  0.077270  0.128967  -0.067226   \n",
       "8945412   4.693841e-15  ...  0.104143  0.103393  -0.046946   \n",
       "8945574  -4.586284e-15  ...  0.105526  0.075433  -0.046178   \n",
       "8945736  -4.574191e-15  ...  0.099154  0.031185  -0.058834   \n",
       "\n",
       "                 local_time  local_hour  location_ID  time_diff  new_event  \\\n",
       "8945088 1989-02-03 19:00:00          19        13770        NaN      False   \n",
       "8945250 1989-02-03 20:00:00          20        13770        1.0      False   \n",
       "8945412 1989-02-03 21:00:00          21        13770        1.0      False   \n",
       "8945574 1989-02-03 22:00:00          22        13770        1.0      False   \n",
       "8945736 1989-02-03 23:00:00          23        13770        1.0      False   \n",
       "\n",
       "         event_ID  global_event_ID  \n",
       "8945088         0          13770_0  \n",
       "8945250         0          13770_0  \n",
       "8945412         0          13770_0  \n",
       "8945574         0          13770_0  \n",
       "8945736         0          13770_0  \n",
       "\n",
       "[5 rows x 73 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>APPAR_TEMP</th>\n",
       "      <th>APPAR_TEMP_R</th>\n",
       "      <th>APPAR_TEMP_U</th>\n",
       "      <th>EFLX_LH_TOT</th>\n",
       "      <th>EFLX_LH_TOT_R</th>\n",
       "      <th>EFLX_LH_TOT_U</th>\n",
       "      <th>...</th>\n",
       "      <th>UWBI_avg</th>\n",
       "      <th>UHI_diff</th>\n",
       "      <th>UWBI_diff</th>\n",
       "      <th>local_time</th>\n",
       "      <th>local_hour</th>\n",
       "      <th>location_ID</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>new_event</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>global_event_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8945088</th>\n",
       "      <td>8945088</td>\n",
       "      <td>1989-02-03 00:00:00</td>\n",
       "      <td>-45.706806</td>\n",
       "      <td>292.5</td>\n",
       "      <td>17.646959</td>\n",
       "      <td>17.637291</td>\n",
       "      <td>18.878418</td>\n",
       "      <td>24.109003</td>\n",
       "      <td>24.298277</td>\n",
       "      <td>-4.979587e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031426</td>\n",
       "      <td>0.086491</td>\n",
       "      <td>-0.130616</td>\n",
       "      <td>1989-02-03 19:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>13770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13770_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945250</th>\n",
       "      <td>8945250</td>\n",
       "      <td>1989-02-03 01:00:00</td>\n",
       "      <td>-45.706806</td>\n",
       "      <td>292.5</td>\n",
       "      <td>15.658759</td>\n",
       "      <td>15.648606</td>\n",
       "      <td>16.951935</td>\n",
       "      <td>7.136189</td>\n",
       "      <td>7.192213</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077270</td>\n",
       "      <td>0.128967</td>\n",
       "      <td>-0.067226</td>\n",
       "      <td>1989-02-03 20:00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>13770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13770_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945412</th>\n",
       "      <td>8945412</td>\n",
       "      <td>1989-02-03 02:00:00</td>\n",
       "      <td>-45.706806</td>\n",
       "      <td>292.5</td>\n",
       "      <td>13.975966</td>\n",
       "      <td>13.966308</td>\n",
       "      <td>15.206352</td>\n",
       "      <td>5.702584</td>\n",
       "      <td>5.747354</td>\n",
       "      <td>4.693841e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104143</td>\n",
       "      <td>0.103393</td>\n",
       "      <td>-0.046946</td>\n",
       "      <td>1989-02-03 21:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>13770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13770_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945574</th>\n",
       "      <td>8945574</td>\n",
       "      <td>1989-02-03 03:00:00</td>\n",
       "      <td>-45.706806</td>\n",
       "      <td>292.5</td>\n",
       "      <td>13.185209</td>\n",
       "      <td>13.176082</td>\n",
       "      <td>14.347800</td>\n",
       "      <td>4.975313</td>\n",
       "      <td>5.014373</td>\n",
       "      <td>-4.586284e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105526</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>-0.046178</td>\n",
       "      <td>1989-02-03 22:00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>13770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13770_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945736</th>\n",
       "      <td>8945736</td>\n",
       "      <td>1989-02-03 04:00:00</td>\n",
       "      <td>-45.706806</td>\n",
       "      <td>292.5</td>\n",
       "      <td>12.749510</td>\n",
       "      <td>12.740907</td>\n",
       "      <td>13.845274</td>\n",
       "      <td>4.594939</td>\n",
       "      <td>4.631013</td>\n",
       "      <td>-4.574191e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099154</td>\n",
       "      <td>0.031185</td>\n",
       "      <td>-0.058834</td>\n",
       "      <td>1989-02-03 23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>13770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13770_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: For each urban grid, identify HWs with positive and negative UHI-HW interactions and then calculate the mean UHI_diff value. Then compare the meteorological conditions (air temperature, humidity, wind, planet boundary layer depth, etc.) between the positive UHI-HW-interaction event and negative UHI-HW-interaction event. ",
   "id": "19a3e1950fd63648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 3.0: Calculate the average event frequency (number of even per year) and average event duration  per location",
   "id": "ec7b69d5de7184f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:46:33.240932Z",
     "start_time": "2024-04-29T19:46:15.308525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named local_hour_adjusted_df\n",
    "# Ensure proper datatypes especially for 'time' column\n",
    "local_hour_adjusted_df['time'] = pd.to_datetime(local_hour_adjusted_df['time'])\n",
    "local_hour_adjusted_df['year'] = local_hour_adjusted_df['time'].dt.year\n",
    "\n",
    "# Group by location_ID, year, and event_ID\n",
    "grouped = local_hour_adjusted_df.groupby(['location_ID', 'year', 'event_ID'])\n",
    "\n",
    "# Calculate event duration in days (count rows and divide by 24)\n",
    "event_duration = grouped.size().div(24).reset_index(name='Duration in Days')\n",
    "\n",
    "# Aggregate data\n",
    "# Number of distinct years per location\n",
    "years_per_location = local_hour_adjusted_df.groupby('location_ID')['year'].nunique()\n",
    "\n",
    "# Number of distinct events per location\n",
    "distinct_events = local_hour_adjusted_df.groupby('location_ID')['event_ID'].nunique()\n",
    "\n",
    "# Average event duration per location\n",
    "average_duration = event_duration.groupby('location_ID')['Duration in Days'].mean()\n",
    "\n",
    "# Normalize event count by the number of years\n",
    "events_per_year = distinct_events / years_per_location\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "hw_freq_by_location = pd.DataFrame({\n",
    "    'location_ID': events_per_year.index,\n",
    "    'Num_events': events_per_year.values,\n",
    "    'Duration_avg': average_duration.values\n",
    "})\n",
    "hw_freq_by_location.head()\n",
    "\n"
   ],
   "id": "c3283f1f8a09b1d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   location_ID  Num_events  Duration_avg\n",
       "0        13770        1.00      3.000000\n",
       "1        14631        1.00      3.000000\n",
       "2        14636        2.48      3.406250\n",
       "3        15206        1.00      3.000000\n",
       "4        15207        1.00      2.833333"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_ID</th>\n",
       "      <th>Num_events</th>\n",
       "      <th>Duration_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13770</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14631</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14636</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15206</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15207</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:51:01.797343Z",
     "start_time": "2024-04-29T19:51:01.787908Z"
    }
   },
   "cell_type": "code",
   "source": "hw_freq_by_location.info()",
   "id": "643bbba753c415eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3704 entries, 0 to 3703\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   location_ID   3704 non-null   int64  \n",
      " 1   Num_events    3704 non-null   float64\n",
      " 2   Duration_avg  3704 non-null   float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 86.9 KB\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:53:41.600108Z",
     "start_time": "2024-04-29T19:53:41.583416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#show % of rows that has Duration_avg < 3.0\n",
    "hw_freq_by_location.query('Duration_avg < 3.0').count()/hw_freq_by_location.count()"
   ],
   "id": "75e0cec8d64087d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_ID     0.014849\n",
       "Num_events      0.014849\n",
       "Duration_avg    0.014849\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T19:54:27.303230Z",
     "start_time": "2024-04-29T19:54:27.294537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "percentage = (hw_freq_by_location['Duration_avg'] < 3.0).mean() * 100\n",
    "percentage"
   ],
   "id": "1f240a3bc8c3577",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4848812095032398"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2ad7b9285ad2838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Step 3.1: define day and night time \n",
    "Daytime: 08:00 to 16:00 local time. (Keer paper)\n",
    "Nighttime: 20:00 to 04:00 local time."
   ],
   "id": "19e78db8f7730494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'local_hour_adjusted_df' is your DataFrame name\n",
    "\n",
    "# Step 1: Define masks for daytime and nighttime\n",
    "daytime_mask = local_hour_adjusted_df['local_hour'].between(8, 16)\n",
    "nighttime_mask = (local_hour_adjusted_df['local_hour'].between(20, 24) |\n",
    "                  local_hour_adjusted_df['local_hour'].between(0, 4))\n",
    "\n",
    "\n"
   ],
   "id": "cb66e99d611684d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 3.2: Calculate the mean UHI_diff value for each event day and night",
   "id": "1a484abbda5b2ed9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to compute averages for UHI_diff based on given mask\n",
    "def compute_uhi_diff_averages(df, mask):\n",
    "    return df[mask].groupby('global_event_ID')['UHI_diff'].mean()\n",
    "\n",
    "# Calculate averages for UHI_diff for daytime and nighttime\n",
    "daytime_uhi_diff_avg = compute_uhi_diff_averages(local_hour_adjusted_df, daytime_mask)\n",
    "nighttime_uhi_diff_avg = compute_uhi_diff_averages(local_hour_adjusted_df, nighttime_mask)"
   ],
   "id": "3519a8445aeb2313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Compute simple averages for all other relevant columns\n",
    "columns_to_average = [ 'UHI_diff', 'UHI', 'UWBI', 'WIND', 'RAIN', 'SNOW', \n",
    "                      'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']\n",
    "uhi_diff_avg_df = local_hour_adjusted_df.groupby('global_event_ID')[columns_to_average].mean()\n",
    "uhi_diff_avg_df.info()"
   ],
   "id": "17a55281dda41475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Add daytime and nighttime UHI_diff averages to the dataframe\n",
    "uhi_diff_avg_df['UHI_diff_daytime'] = daytime_uhi_diff_avg\n",
    "uhi_diff_avg_df['UHI_diff_nighttime'] = nighttime_uhi_diff_avg\n",
    "\n",
    "# The resulting DataFrame, 'simple_averages_df', now includes the requested columns"
   ],
   "id": "c842e3bc28aec06d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "uhi_diff_avg_df.info()\n",
    "uhi_diff_avg_df.head(300)\n"
   ],
   "id": "7f6c2952d0e1329a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "uhi_diff_avg_df.query('UHI_diff< 0').count()",
   "id": "a715bc6b1f32ed0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Data Analysis\n",
   "id": "85f35b2ca3e59534"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "# Assuming 'uhi_diff_avg_df' is already loaded in your environment\n",
    "\n",
    "# Separate the data into two groups\n",
    "negative_uhi_diff = uhi_diff_avg_df[uhi_diff_avg_df['UHI_diff'] < 0]\n",
    "positive_uhi_diff = uhi_diff_avg_df[uhi_diff_avg_df['UHI_diff'] > 0]\n",
    "\n",
    "# Define non-UHI columns\n",
    "non_uhi_columns = ['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']\n",
    "\n",
    "# # Descriptive Statistics\n",
    "# print(\"Descriptive Statistics for UHI_diff < 0:\")\n",
    "# print(negative_uhi_diff[non_uhi_columns].describe())\n",
    "# print(\"\\nDescriptive Statistics for UHI_diff > 0:\")\n",
    "# print(positive_uhi_diff[non_uhi_columns].describe())"
   ],
   "id": "c827bc5d8ee8d858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "negative_uhi_diff",
   "id": "6cc28a04052c50fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "local_hour_adjusted_df.query('global_event_ID == \"15782_1\"')",
   "id": "863ab8deffda23d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Correlation Analysis\n",
    "correlations = uhi_diff_avg_df[non_uhi_columns + ['UHI_diff']].corr()['UHI_diff']\n",
    "print(\"\\nCorrelations with UHI_diff:\")\n",
    "print(correlations)\n"
   ],
   "id": "a571a518e071fe46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Which Variable is contributing to the UHI_diff",
   "id": "206b4440e8b3fa7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 5.1: Using logistic regression",
   "id": "f1a6191335b5828f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'uhi_diff_avg_df' is already in your environment\n",
    "\n",
    "# Prepare the data\n",
    "X = uhi_diff_avg_df[['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']]\n",
    "y = (uhi_diff_avg_df['UHI_diff'] > 0).astype(int)  # Create a binary target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Summarize the background data using shap.kmeans\n",
    "background_data = shap.kmeans(X_train_scaled, k=30)  # Summarize with 30 representative clusters\n",
    "\n",
    "# Create SHAP values using KernelExplainer with the summarized background\n",
    "explainer = shap.KernelExplainer(model.predict_proba, background_data, link=\"logit\", n_jobs = 32)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Plot the SHAP values for the positive class\n",
    "shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X.columns, plot_type=\"bar\")\n",
    "\n",
    "\n"
   ],
   "id": "b0b3f1a4b096bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "# Assuming 'uhi_diff_avg_df' is already in your environment\n",
    "\n",
    "# Prepare the data\n",
    "X = uhi_diff_avg_df[['UWBI', 'WIND', 'RAIN', 'SNOW', 'Q2M_R', 'Q2M_U', 'VAPOR_PRES_R', 'VAPOR_PRES_U']]\n",
    "y = uhi_diff_avg_df['UHI']  # Assuming you want to predict UHI_diff directly\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create SHAP values\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summarize the SHAP values in a plot to show the impact of each feature\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ],
   "id": "146cca73d087ff61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "strong positive correlation UHI and HW\n",
    "negative event \n",
    "insignficant event \n",
    "global map \n",
    "for each grid # postive and negative interaction \n"
   ],
   "id": "1361be684a2f3d35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
