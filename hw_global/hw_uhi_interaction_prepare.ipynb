{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T04:21:31.602136Z",
     "start_time": "2024-04-11T04:21:31.366203Z"
    }
   },
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def spatial_mean(da):\n",
    "    \"\"\"Computes the spatial mean if lat and lon dimensions are present.\"\"\"\n",
    "    if 'lat' in da.dims and 'lon' in da.dims:\n",
    "        return da.mean(dim=['lat', 'lon'])\n",
    "    return da\n",
    "\n",
    "# Convert cftime.DatetimeNoLeap to numpy.datetime64\n",
    "def convert_time(ds):\n",
    "    ds['time'] = [pd.Timestamp(time.strftime()) for time in ds['time'].values]\n",
    "    return ds"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#  Understand land surface data\n",
    "##  1.  Load the landsurface data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b8fb4778f7dbd84"
  },
  {
   "cell_type": "code",
   "source": [
    "fsurdat: str = \"/home/jguo/projects/cesm/inputdata/lnd/clm2/surfdata_map/release-clm5.0.18/surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_c190214.nc\"\n",
    "# Load the dataset\n",
    "ds_sur = xr.open_mfdataset(fsurdat)\n",
    "ds_sur"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T04:21:38.393710Z",
     "start_time": "2024-04-11T04:21:37.882700Z"
    }
   },
   "id": "90b0a9c6c8e2b656",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  2. Assign coordinate value to be actual lat and lon degree not the index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3df0cd8f58fe9dd"
  },
  {
   "cell_type": "code",
   "source": [
    "print(ds_sur.coords)\n",
    "print(ds_sur.dims)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T04:22:22.290853Z",
     "start_time": "2024-04-11T04:22:22.283982Z"
    }
   },
   "id": "b7f9df8e4ce79cfa",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract a column of unique latitudes and a row of unique longitudes as raw arrays\n",
    "# convert to float32 to match the hw temperature data later. this gave me a lot of trouble\n",
    "unique_lats = ds_sur['LATIXY'].isel(lsmlon=0).data.astype('float32')  # Extracting the raw array for latitudes\n",
    "unique_lons = ds_sur['LONGXY'].isel(lsmlat=0).data.astype('float32')  # Extracting the raw array for longitudes\n",
    "\n",
    "# Assign these raw arrays as new coordinates (without specifying old dimensions)\n",
    "ds_sur = ds_sur.assign_coords(lat=('lsmlat', unique_lats), lon=('lsmlon', unique_lons))\n",
    "\n",
    "# Swap 'lsmlat' and 'lsmlon' dimensions with 'lat' and 'lon'\n",
    "ds_sur = ds_sur.swap_dims({'lsmlat': 'lat', 'lsmlon': 'lon'})\n",
    "\n",
    "# Drop the original 'lsmlat' and 'lsmlon' dimensions if they are no longer needed\n",
    "ds_sur = ds_sur.drop_vars(['lsmlat', 'lsmlon'], errors='ignore')\n",
    "\n",
    "\n",
    "# Now, your DataArray or any variable extracted from ds_sur should have 1 dimensionional (1D) lat and lon coordinates\n",
    "total_urban_pct = ds_sur['PCT_URBAN']  # Example variable\n",
    "print(total_urban_pct)\n",
    "\n",
    "# At this point, the dataset dimensions will be based on actual latitude and longitude values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33e1a6a29ddef99e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  3.  Determine which grid is Urban"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17602931b2c41f8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Access the PCT_URBAN variable\n",
    "pct_urban = ds_sur['PCT_URBAN']\n",
    "pct_urban\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6d0d6f99429620c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sum across the 'numurbl' dimension to get total urban coverage\n",
    "\n",
    "# https://bb.cgd.ucar.edu/cesm/threads/proportion-of-cities-in-surface-data.8046/\n",
    "# PCT_URBAN is the percent of each urban density type. The density types in order are\n",
    "# tall building district (TBD), high density (HD), and medium density (MD).\n",
    "# If you change those percentages, e.g, increase them, then you'll need to decrease\n",
    "# some other surface type (e.g., PCT_NATVEG, PCT_CROP, PCT_LAKE, etc.).\n",
    "# The sum of PCT_URBAN, PCT_NATVEG, PCT_CROP, PCT_LAKE, PCT_GLACIER, PCT_WETLAND needs to be 100%.\n",
    "# PCT_URBAN has multiple layers for different urban density types, sum across the 'numurbl' dimension to get total urban coverage\n",
    "total_urban_pct = pct_urban.sum(dim='numurbl')\n",
    "print(total_urban_pct)\n",
    "total_urban_pct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e3f4a1c351a7f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  4.  Create a mask for the urban grid, the mask is variable masked_urban_areas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36dae7d38b6a72c0"
  },
  {
   "cell_type": "code",
   "source": [
    "# Use .where() to assign 1 to urban cells (where total urban percentage > 2%) and NaN to others\n",
    "urban_grid_mask = total_urban_pct.where(total_urban_pct > 0.1, other=np.nan)\n",
    "\n",
    "# Now, replace all non-NaN values (which indicate urban areas) with 1\n",
    "# This is done by utilizing the np.where function from the numpy library to replace non-NaN (i.e., urban) values with 1\n",
    "urban_grid_binary = xr.where(urban_grid_mask.notnull(), 1, urban_grid_mask)\n",
    "\n",
    "\n",
    "\n",
    "def find_top_urban_areas(masked_urban_areas):\n",
    "    # Convert the stacked DataArray to a pandas DataFrame\n",
    "    df = masked_urban_areas.stack(z=('lat', 'lon')).to_dataframe(name='urban_pct')\n",
    "\n",
    "    # Use the nlargest method on the DataFrame to find the top 50 values\n",
    "    top_urban_areas_df = df.nlargest(50, 'urban_pct')\n",
    "    # Drop the redundant 'lsmlat' and 'lsmlon' columns\n",
    "    top_urban_areas_df = top_urban_areas_df.drop(columns=['lat', 'lon'])\n",
    "\n",
    "    return top_urban_areas_df\n",
    "\n",
    "\n",
    "# Call the function with the masked_urban_areas variable\n",
    "top_urban_areas = find_top_urban_areas(urban_grid_mask)\n",
    "print(top_urban_areas)\n",
    "\n",
    "print( \"percentage grid cells are urban:\", urban_grid_mask.count().compute().item() * 100 / (192 * 288), \"%\")\n",
    "\n",
    "# Rename dimensions in urban_grid_binary to match those in HW\n",
    "# urban_grid_binary = urban_grid_binary.rename({'lsmlat': 'lat', 'lsmlon': 'lon'})\n",
    "urban_grid_binary\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eb7c31cc309284e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4aa92ff17d3f6a0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  5.  Plot the urban grid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cab49ea14d86c3ea"
  },
  {
   "cell_type": "code",
   "source": [
    "urban_grid_mask.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29a3fab7f28d453a",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "\n",
    "# Get longitude and latitude information from the dataset\n",
    "longitude = ds_sur['LONGXY']\n",
    "latitude = ds_sur['LATIXY']\n",
    "\n",
    "# Plotting the urban areas that meet the condition\n",
    "# Note: urban_grid_mask already has values below 2% filtered out, so we use it directly\n",
    "plt.pcolormesh(longitude, latitude, urban_grid_mask, transform=ccrs.PlateCarree(), cmap='cool') #cmap='cool') cmap='Reds')\n",
    "\n",
    "plt.colorbar(label='Total Urban Percentage')\n",
    "plt.title('Urban Areas with More Than 2% Urban Coverage')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d1849f7dec023e3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  6.  Use TSA_U to determine urban grid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3348f88518325a"
  },
  {
   "cell_type": "code",
   "source": [
    "one_monthly_file = '/tmpdata/summerized_data/i.e215.I2000Clm50SpGs.hw_production.02/sub_sample/i.e215.I2000Clm50SpGs.hw_production.02.clm2.h0.1985-01.nc'\n",
    "ds_monthly = xr.open_dataset(one_monthly_file)\n",
    "ds_monthly.TSA_U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:06.481101Z",
     "start_time": "2024-03-09T21:39:06.366158Z"
    }
   },
   "id": "dcf15014026a089a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#put all variables in ds_monthly in a list\n",
    "ds_monthly_vars = list(ds_monthly.data_vars)\n",
    "ds_monthly_vars\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T18:02:21.208784Z",
     "start_time": "2024-03-10T18:02:21.201994Z"
    }
   },
   "id": "ff3996026beaabf9",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "ds_monthly.TSA_U.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74484eddc4776e98",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#urban_grid_binary_from_TSA_U = xr.where(ds_monthly.TSA_U > 0 , 1, urban_grid_mask)\n",
    "#print in 2digits prcecision percentage of urban grid cells\n",
    "print(f\"urban: {ds_monthly.TSA_U.notnull().sum().compute().item() *100/(288*192):.1f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:11:56.561182Z",
     "start_time": "2024-03-09T21:11:56.548894Z"
    }
   },
   "id": "3e6347efdcef2fb3",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Select the 'TSA_U' variable and get the non-null mask\n",
    "# Using the .isel() method to select the first time slice\n",
    "urban_non_null_mask = ds_monthly['TSA_U'].isel(time=0).notnull().drop('time')   \n",
    "urban_non_null_mask\n",
    "\n",
    "# # Apply the mask to filter the dataset\n",
    "# ds_filtered = ds_monthly.where(urban_non_null_mask, drop=True)\n",
    "# \n",
    "# # Now 'ds_filtered' contains only the grid cells where 'TSA_U' is not null\n",
    "# ds_filtered"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:18.917953Z",
     "start_time": "2024-03-09T21:39:18.898571Z"
    }
   },
   "id": "5a4cdda8ef2653b6",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#  Define HW Temporal filter\n",
    "##  1.  Load the daily(h1 tape) simulation temperature data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ba266ef51e6add"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset, I copied the daily h1 files to /home/jguo/process_data/i.e215.I2000Clm50SpGs.hw_production.02/daily_raw it is 137G in total\n",
    "I run the utils/extract_var_save.py to extract the subset of variables from the above directory\n",
    "and save the data to /home/jguo/process_data/i.e215.I2000Clm50SpGs.hw_production.02/summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6354f538fd7d427b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99caa8a7649aee7b"
  },
  {
   "cell_type": "code",
   "source": [
    "hw_summary_dir = '/home/jguo/process_data/i.e215.I2000Clm50SpGs.hw_production.02/summary'\n",
    "hw_output_file = 'i.e215.I2000Clm50SpGs.hw_production.02.clm2.h1.hwdays.nc'\n",
    "hw_out_file_path = os.path.join(hw_summary_dir, hw_output_file)\n",
    "#check if hw_file exists\n",
    "hw_file_exist = os.path.isfile(hw_out_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:27.647676Z",
     "start_time": "2024-03-09T21:39:27.643215Z"
    }
   },
   "id": "5fdd763de933f77f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "319a44e0139fe96e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  2   For each grid cell the find time periods that satisfy the HW definition. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9afae2add188f43f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the definition from the US National Weather Service (NWS): three or more consecutive days of maximumtemperature reaching at least 90 ◦F (32.2 ◦C). \n",
    "We consider that, in each grid cell (a size on the order of 100 × 100 km), its rural sub-grid represents a local background environment for the city. \n",
    "Therefore, for each city we use its rural 2m-height temperature (T2m,rural) to define HWs.\n",
    "\n",
    "We use this variable in the daily h1 file: TREFMXAV_R:long_name = \"Rural daily maximum of average 2-m temperature\" ;"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bb6126228700975"
  },
  {
   "cell_type": "code",
   "source": [
    "fahrenheit_threshold = 90\n",
    "kelvin_threshold = (fahrenheit_threshold - 32) * (5/9) + 273.15"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T22:07:25.364662Z",
     "start_time": "2024-03-09T22:07:25.360688Z"
    }
   },
   "id": "65017279f54675f9",
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#if hw_file does not exist, then we need to run the following code to create the hw_file\n",
    "if not hw_file_exist:\n",
    "    # Open the NetCDF file containing the rural daily maximum of average 2-m temperature\n",
    "    hw_input_file = 'i.e215.I2000Clm50SpGs.hw_production.02.clm2.h1.TSA_UR_TREFMXAV_R.nc'\n",
    "    hw_input_file_path = os.path.join(hw_summary_dir, hw_input_file)\n",
    "    ds_hw = xr.open_dataset(hw_input_file_path)\n",
    "    ds_hw\n",
    "    # Define the threshold temperature in Kelvin\n",
    "    # Convert 90 degrees Fahrenheit to Kelvin\n",
    "    fahrenheit_threshold = 90\n",
    "    kelvin_threshold = (fahrenheit_threshold - 32) * (5/9) + 273.15\n",
    "\n",
    "    # Define a function to apply on each grid cell to detect heatwaves\n",
    "    def detect_heatwave(tsa_r_np):\n",
    "        # Ensure tsa_r_np is a 1D array for simplicity\n",
    "        tsa_r_np = np.atleast_1d(tsa_r_np)\n",
    "        hw = np.full(tsa_r_np.shape, np.nan)  # Initialize HW with NaN\n",
    "\n",
    "        # Check for heatwaves\n",
    "        for i in range(2, len(tsa_r_np)):\n",
    "            if (tsa_r_np[i-2] > kelvin_threshold and\n",
    "                    tsa_r_np[i-1] > kelvin_threshold and\n",
    "                    tsa_r_np[i] > kelvin_threshold):\n",
    "                hw[i-2:i+1] = 1  # Mark all three days as heatwave\n",
    "\n",
    "        return hw\n",
    "\n",
    "    # Use apply_ufunc to apply the detect_heatwave function across the dataset\n",
    "    hw = xr.apply_ufunc(\n",
    "        detect_heatwave, ds_hw['TREFMXAV_R'],\n",
    "        input_core_dims=[['time']],  # Specify the core dimension\n",
    "        output_core_dims=[['time']],  # Ensure output has the same core dimension as input\n",
    "        vectorize=True,  # Enable broadcasting and looping over other dimensions\n",
    "        output_dtypes=[float]  # Specify the output data type\n",
    "    )\n",
    "    # Optional: save the modified dataset to a new NetCDF file\n",
    "    # Assign the HW data back to the original dataset as a new variable\n",
    "    ds_hw['HW'] = hw\n",
    "    ds_hw.to_netcdf(hw_out_file_path)   \n",
    "else:\n",
    "    # Load the existing HW data\n",
    "    ds_hw =xr.open_dataset(hw_out_file_path)\n",
    "    hw = ds_hw['HW']    \n",
    "\n",
    "print(hw)\n",
    "hw\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:31.696356Z",
     "start_time": "2024-03-09T21:39:31.495885Z"
    }
   },
   "id": "8c21e05c33bb28cd",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds_hw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f4123b1bba0f415",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  3.  The outcome should be 2D array with 1 for HW day and 0 for non HW day, the xarray should daily time dimension."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4bbe181fe387502"
  },
  {
   "cell_type": "code",
   "source": [
    "#todo read hw from the output file hw_file_path\n",
    "hw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a7ad36075a7869",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  4.  Interecting the HW grid with the Urban grid, we can find the HW days for each urban grid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "481fcf172aaa4127"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(hw.lon.equals(urban_non_null_mask.lon))\n",
    "print(hw.lat.equals(urban_non_null_mask.lat))\n",
    "\n",
    "print(type(urban_non_null_mask.lon))\n",
    "print(type(hw.lon))\n",
    "print('\\nurban_non_null_mask.lon:\\n', urban_non_null_mask.lon, '\\n')\n",
    "print(\"hw.lon:\\n\", hw.lon, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:41.798223Z",
     "start_time": "2024-03-09T21:39:41.789159Z"
    }
   },
   "id": "b745a785b1ad5b59",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds_hw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T23:03:39.460612Z",
     "start_time": "2024-03-09T23:03:39.379503Z"
    }
   },
   "id": "761b1f3f63fbc5e5",
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "urban_non_null_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:39:51.766843Z",
     "start_time": "2024-03-09T21:39:51.757075Z"
    }
   },
   "id": "9c9df2d2fadf4eb1",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply the mask to filter the dataset\n",
    "# Apply the urban mask across all time points without dropping any\n",
    "ds_hw_filtered = ds_hw.where(urban_non_null_mask.broadcast_like(ds_hw), drop=False)\n",
    "ds_hw_filtered"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:40:51.986697Z",
     "start_time": "2024-03-09T21:40:06.879583Z"
    }
   },
   "id": "9eb85ec14e4aa641",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# report "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2066578e29470b8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7ba2cc45784fed1"
  },
  {
   "cell_type": "code",
   "source": [
    "hw = ds_hw_filtered['HW']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2add0904a74fdb7",
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Debug\n",
    "# check in hw, how many grid cells has hw == 1 on the first day\n",
    "hw_i = ds_hw_filtered.isel(time=2)\n",
    "hw_i['HW'].plot()\n",
    "\n",
    "# Select data where 'HW' equals 1\n",
    "hw_condition_met = hw_i.where(hw_i['HW'] == 1, drop=True).drop('time')\n",
    "hw_condition_met['HW'].sum()\n",
    "hw_condition_met"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T22:04:41.262886Z",
     "start_time": "2024-03-09T22:04:40.868269Z"
    }
   },
   "id": "57dd60a78e8bcba3",
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Convert the filtered dataset to a pandas DataFrame\n",
    "df = ds_hw_filtered.isel(time=slice(1, 2)).to_dataframe()\n",
    "\n",
    "# Reset the index if 'lat' and 'lon' are part of a MultiIndex\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "# Filter the DataFrame for rows where 'HW' equals 1\n",
    "df_hw_1 = df[df['HW'] == 1]\n",
    "\n",
    "# Sort the filtered DataFrame by 'lat' and then by 'lon'\n",
    "df_hw_1_sorted = df_hw_1.sort_values(by=['lat', 'lon'])\n",
    "\n",
    "# Set the maximum number of columns to display\n",
    "pd.set_option('display.max_columns', 50)  # Adjust the number as needed\n",
    "\n",
    "# Print the sorted, filtered DataFrame\n",
    "df_hw_1_sorted\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T22:19:22.998334Z",
     "start_time": "2024-03-09T22:19:22.973634Z"
    }
   },
   "id": "751e6030946bddc1",
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Sum over the spatial dimensions to count the number of HW == 1 cells for each day\n",
    "# Assuming 'lat' and 'lon' are the names of your spatial dimensions\n",
    "daily_hw_urban_count = hw.sum(dim=['lat', 'lon']).compute()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:44:57.685689Z",
     "start_time": "2024-03-09T21:44:45.402832Z"
    }
   },
   "id": "4354985cddd1d88a",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f108451589e465"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# daily_hw_urban_count_computed now contains the daily count of urban grid cells with HW == 1\n",
    "print(daily_hw_urban_count)\n",
    "\n",
    "#print out daily count of urban grid cells for days that has count > 1\n",
    "hw_dates= daily_hw_urban_count.where(daily_hw_urban_count > 1, drop=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:45:03.326357Z",
     "start_time": "2024-03-09T21:45:03.313155Z"
    }
   },
   "id": "896be5edd8ca4f2",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#convert hw_dates to df\n",
    "df_hw_dates = hw_dates.to_dataframe()\n",
    "df_hw_dates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T23:16:05.341465Z",
     "start_time": "2024-03-09T23:16:05.331843Z"
    }
   },
   "id": "d62a500d7b2d4836",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#save hw_dates to a file\n",
    "hw_dates.to_netcdf('/home/jguo/process_data/i.e215.I2000Clm50SpGs.hw_production.02/summary/hw_dates.nc')   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T21:45:12.388980Z",
     "start_time": "2024-03-09T21:45:11.984735Z"
    }
   },
   "id": "1e2134377978561",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 1: Mask the hw dataset to only include grid cells marked as urban in urban_grid_binary\n",
    "hw_urban = hw.where(urban_grid_binary == 1)\n",
    "\n",
    "# Step 2: Reduce the hw_urban dataset along spatial dimensions to identify any heatwave occurrence in urban areas for each date\n",
    "# The reduction will result in a DataArray where each time point has a value of True if any urban grid cell experienced a heatwave, and False otherwise\n",
    "hw_urban_any = hw_urban.reduce(np.any, dim=['lat', 'lon'])\n",
    "# Ensure hw_urban_any is computed if it's a Dask array and convert time to a pandas Timestamp\n",
    "hw_urban_any_computed = hw_urban_any.compute()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f58ed34f9349471f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Filter the time coordinate where there's at least one urban heatwave, which gives us the dates we're interested in\n",
    "hw_dates = hw_urban_any_computed.time.where(hw_urban_any_computed, drop=True)\n",
    "\n",
    "# Convert the hw_dates xarray DataArray to a list of pandas Timestamps for easier interpretation and use\n",
    "hw_dates_list = pd.to_datetime(hw_dates.values).tolist()\n",
    "\n",
    "# Print the resulting list of dates\n",
    "print(hw_dates_list)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63d066cbb61de358",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(hw_urban)\n",
    "hw_urban.isel(time=0).drop(['time'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ae75a1a3e21378b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "###  4.1 make sure we do north and south hemisphere correctly\n",
    "##  5. Compute the \"HW days for Any grid\" list of dates. I need to load file on that day if any grid has a HW day.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ab8229bc065dfe1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Check when HW == 1 for any grid cell and reduce the data array to a 1D time series\n",
    "heatwave_days = hw_urban.any(dim=('lat', 'lon')).compute() #reduce the data array to a 1D time series\n",
    "\n",
    "# Extract the dates where the condition is True\n",
    "heatwave_dates = hw_urban['time'].where(heatwave_days, drop=True)\n",
    "\n",
    "# Print the dates\n",
    "print(heatwave_dates.values)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c520434dc5515c1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Count the number of HW days\n",
    "num_hw_days = heatwave_dates.size\n",
    "\n",
    "print(\"Number of Heatwave Days:\", num_hw_days)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d05e53c915fd1b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  5.1  further descrease that list by \n",
    "\n",
    "\n",
    "#  Process hourly simulation output data\n",
    "##  1. On a given hourly time slice, compute the UHI for all grid cells that are urban and HW (marked as HW == 1) (grid level hourly)\n",
    "###  1.1 TSA_U > 0 is urban cell\n",
    "###  1.2 HW is defined with above code. Grid level daily. \n",
    "##  2. For each year on summer time-series, Compute summer average UHI for each given hour of the day for a given urban cell (Grid level summer monthly)\n",
    "###  2.1 Compute summer average UHI for given hour of day for a given urban cell using monthly data\n",
    "##  3. For each year on summer time-series, Compute UHI for a given urban and HW cell. (grid level hourly)\n",
    "###  3.1 Compute UHI for a given urban and HW cell\n",
    "##  4. Compute \"UHI interaction\", UHI with HW - UHI (Grid level hourly)\n",
    "###   4.1 Compute summer average UHI for each grid cell\n",
    "###   4.2 Compute UHI interaction for each grid cell: HW grid level UHI - grid level Summer average UHI for each given hour of day\n",
    "\n",
    "#  Explain the \"UHI interaction\" (defined above) \n",
    "## 1.  ML model left hand side (LHS) UHI interaction. Right hand side (RHS) factors. \n",
    "###  1.1 LHS is hourly single cell data for \"UHI interaction\"\n",
    "###  1.2 RHS contains lon, lat, hourly time, humidity, distance to shore, different part of day, surface roughness, albedo (R and U), precipitation, vegetation (TODO: study different types of vegetation, pct, etc.) \n",
    "###  1.3 We want to figure out contribution for each RHS factor to the LHS UHI interaction. Plan to use SHAP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  1. Load the hourly simulation temperature data from \"HW days for Any grid\" list of dates\n",
    "##  2. Convert UTC to local time \n",
    "##  3. Compute the UHI for 2m air T for each HW grid\n",
    "##  4. Compute the UWBI for surface for each HW grid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  1.  feature engineering, the right hand side factors\n",
    "###  1.1  humidity(precipitation))\n",
    "###  1.2  distance to shore (what about big lakes)?\n",
    "###  1.3  Different part of day (morning, afternoon, evening, night)\n",
    "###  1.4  Surface roughness, albedo, vegetation\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2241aaf18747561"
  },
  {
   "cell_type": "markdown",
   "source": [
    "SHAP values explain the prediction of an instance by computing the contribution of each feature to the prediction. The SHAP value of a feature for a particular prediction indicates the impact of that feature being introduced into a conditional expectation model of the prediction. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da28d33ec02e5c8"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5cec0479239b8e4c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4098e5a7c5e81016"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
