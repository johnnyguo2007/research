{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b141e7363d2170",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Read the Zarr dataset using Xarray with automatic chunking for Dask\n",
    "ds = xr.open_zarr('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/zarr2/HW', chunks='auto')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "core_vars = ['UHI', 'UBWI', 'HW']\n",
    "ds = ds[core_vars]\n",
    "ds\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6292251548100df6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "#find grid cell and date that HW is not True    \n",
    "# Assuming 'HW' is a boolean variable in your dataset indicating heat waves\n",
    "# We want to find where HW is False (not True)\n",
    "ds_filter = ds.sel(time=slice('1985-01-02', '1985-12-31'))\n",
    "\n",
    "\n",
    "# Select the grid cell and the variables, then convert to DataFrame\n",
    "\n",
    "df = ds_filter.to_dataframe(['lat', 'lon', 'time']).dropna()\n",
    "# df = ds_filter.to_dataframe().dropna()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e26ff883a202679",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1985-1988 data to /Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet/1985_1988_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Read the Zarr dataset using Xarray with automatic chunking for Dask\n",
    "ds = xr.open_zarr('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/zarr2/HW', chunks='auto')\n",
    "\n",
    "# Select core variables\n",
    "core_vars = ['UHI', 'UBWI', 'HW']\n",
    "ds = ds[core_vars]\n",
    "\n",
    "# Define start and end year of your dataset\n",
    "start_year = 1985\n",
    "end_year = 1988\n",
    "\n",
    "# Define the number of years to process in each chunk\n",
    "years_per_chunk = 12\n",
    "\n",
    "for start_chunk_year in range(start_year, end_year + 1, years_per_chunk):\n",
    "    # Initialize an empty list to hold DataFrames for each year\n",
    "    df_list = []\n",
    "\n",
    "    # Determine the end year for the current chunk, ensuring it does not exceed the end_year\n",
    "    end_chunk_year = min(start_chunk_year + years_per_chunk - 1, end_year)\n",
    "\n",
    "    for year in range(start_chunk_year, end_chunk_year + 1):\n",
    "        # Select the data for the current year\n",
    "        ds_year = ds.sel(time=slice(f'{year}-01-01', f'{year}-12-31'))\n",
    "\n",
    "        # Convert to DataFrame without resetting the index\n",
    "        df_year = ds_year.to_dataframe(['lat', 'lon', 'time']).dropna()\n",
    "\n",
    "        # Append the DataFrame for the current year to the list\n",
    "        df_list.append(df_year)\n",
    "\n",
    "    # Concatenate all DataFrames in the list to create a single DataFrame for the 10-year chunk\n",
    "    df_chunk = pd.concat(df_list)\n",
    "\n",
    "    # Define the path to the Parquet file for this 10-year chunk\n",
    "    parquet_file_path = f'/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/parquet/HW_{start_chunk_year}_{end_chunk_year}.parquet'\n",
    "\n",
    "    # Write the 10-year chunk DataFrame to a Parquet file\n",
    "    df_chunk.to_parquet(parquet_file_path, engine='pyarrow', index=True)\n",
    "\n",
    "    print(f'Saved {start_chunk_year}-{end_chunk_year} data to {parquet_file_path}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T05:12:11.574131Z",
     "start_time": "2024-03-19T05:10:58.020944Z"
    }
   },
   "id": "bb151cdcafd04d43",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
