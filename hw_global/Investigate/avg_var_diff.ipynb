{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6181728 entries, 0 to 6181727\n",
      "Data columns (total 75 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   time             datetime64[ns]\n",
      " 1   lat              float32       \n",
      " 2   lon              float32       \n",
      " 3   APPAR_TEMP       float32       \n",
      " 4   APPAR_TEMP_R     float32       \n",
      " 5   APPAR_TEMP_U     float32       \n",
      " 6   EFLX_LH_TOT      float32       \n",
      " 7   EFLX_LH_TOT_R    float32       \n",
      " 8   EFLX_LH_TOT_U    float32       \n",
      " 9   FGR              float32       \n",
      " 10  FGR_R            float32       \n",
      " 11  FGR_U            float32       \n",
      " 12  FIRA             float32       \n",
      " 13  FIRA_R           float32       \n",
      " 14  FIRA_U           float32       \n",
      " 15  FIRE             float32       \n",
      " 16  FIRE_R           float32       \n",
      " 17  FIRE_U           float32       \n",
      " 18  FLDS             float32       \n",
      " 19  FSA              float32       \n",
      " 20  FSA_R            float32       \n",
      " 21  FSA_U            float32       \n",
      " 22  FSDS             float32       \n",
      " 23  FSH              float32       \n",
      " 24  FSH_R            float32       \n",
      " 25  FSH_U            float32       \n",
      " 26  HEAT_FROM_AC     float32       \n",
      " 27  HIA              float32       \n",
      " 28  HIA_R            float32       \n",
      " 29  HIA_U            float32       \n",
      " 30  PBOT             float32       \n",
      " 31  Q2M              float32       \n",
      " 32  Q2M_R            float32       \n",
      " 33  Q2M_U            float32       \n",
      " 34  QBOT             float32       \n",
      " 35  Qstor            float32       \n",
      " 36  RAIN             float32       \n",
      " 37  SNOW             float32       \n",
      " 38  SOILWATER_10CM   float32       \n",
      " 39  TBOT             float32       \n",
      " 40  THBOT            float32       \n",
      " 41  TSA              float32       \n",
      " 42  TSA_R            float32       \n",
      " 43  TSA_U            float32       \n",
      " 44  TSKIN            float32       \n",
      " 45  TSKIN_R          float32       \n",
      " 46  TSKIN_U          float32       \n",
      " 47  U10              float32       \n",
      " 48  URBAN_HEAT       float32       \n",
      " 49  VAPOR_PRES       float32       \n",
      " 50  VAPOR_PRES_R     float32       \n",
      " 51  VAPOR_PRES_U     float32       \n",
      " 52  WASTEHEAT        float32       \n",
      " 53  WBA              float32       \n",
      " 54  WBA_R            float32       \n",
      " 55  WBA_U            float32       \n",
      " 56  WIND             float32       \n",
      " 57  HW98             bool          \n",
      " 58  location_ID      float64       \n",
      " 59  event_ID         float64       \n",
      " 60  global_event_ID  object        \n",
      " 61  hour             int32         \n",
      " 62  month            int32         \n",
      " 63  year             int32         \n",
      " 64  local_time       datetime64[ns]\n",
      " 65  local_hour       int32         \n",
      " 66  UHI              float32       \n",
      " 67  UWBI             float32       \n",
      " 68  UHI_avg          float32       \n",
      " 69  UWBI_avg         float32       \n",
      " 70  UHI_diff         float32       \n",
      " 71  UWBI_diff        float32       \n",
      " 72  TOPO             float64       \n",
      " 73  KGClass          object        \n",
      " 74  KGMajorClass     object        \n",
      "dtypes: bool(1), datetime64[ns](2), float32(62), float64(3), int32(4), object(3)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "hw = pd.read_feather('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.05/research_results/summary/local_hour_adjusted_variables_HW98.feather')\n",
    "hw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hw = pd.read_feather('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.05/research_results/summary/no_hw_HW98.feather')\n",
    "no_hw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_cat = pd.read_feather('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.05/research_results/summary/kg_category_location_ID.feather')\n",
    "kg_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataframes\n",
    "\n",
    "# List of variables to average\n",
    "variables_to_average = [\n",
    "    'APPAR_TEMP', 'APPAR_TEMP_R', 'APPAR_TEMP_U', 'EFLX_LH_TOT', 'EFLX_LH_TOT_R', 'EFLX_LH_TOT_U',\n",
    "    'FGR', 'FGR_R', 'FGR_U', 'FIRA', 'FIRA_R', 'FIRA_U', 'FIRE', 'FIRE_R', 'FIRE_U',\n",
    "    'FLDS', 'FSA', 'FSA_R', 'FSA_U', 'FSDS', 'FSH', 'FSH_R', 'FSH_U', 'HEAT_FROM_AC',\n",
    "    'HIA', 'HIA_R', 'HIA_U', 'PBOT', 'Q2M', 'Q2M_R', 'Q2M_U', 'QBOT', 'Qstor',\n",
    "    'RAIN', 'SNOW', 'SOILWATER_10CM', 'TBOT', 'THBOT', 'TSA', 'TSA_R', 'TSA_U',\n",
    "    'TSKIN', 'TSKIN_R', 'TSKIN_U', 'U10', 'URBAN_HEAT', 'VAPOR_PRES', 'VAPOR_PRES_R', 'VAPOR_PRES_U',\n",
    "    'WASTEHEAT', 'WBA', 'WBA_R', 'WBA_U', 'WIND', 'UHI', 'UWBI'\n",
    "]\n",
    "\n",
    "# # Ensure 'location_ID' is of the same type in both dataframes\n",
    "# no_hw['location_ID'] = no_hw['location_ID'].astype(int)\n",
    "# kg_cat['location_ID'] = kg_cat['location_ID'].astype(int)\n",
    "\n",
    "# Join the dataframes\n",
    "merged_df = pd.merge(no_hw, kg_cat[['location_ID', 'KGMajorClass']], on='location_ID', how='left')\n",
    "\n",
    "# Group by KGMajorClass and local_hour, then compute averages\n",
    "grouped_df = merged_df.groupby(['KGMajorClass', 'local_hour'])[variables_to_average].mean().reset_index()\n",
    "\n",
    "# Print the first few rows of the result\n",
    "print(grouped_df.head())\n",
    "\n",
    "# Print the shape of the resulting dataframe\n",
    "print(f\"Shape of the resulting dataframe: {grouped_df.shape}\")\n",
    "\n",
    "# # Save the result to a CSV file\n",
    "# output_path = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/summary/kg_class_hourly_averages.csv'\n",
    "# grouped_df.to_csv(output_path, index=False)\n",
    "# print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# # Optional: Calculate and print memory usage\n",
    "# memory_usage = grouped_df.memory_usage(deep=True).sum() / 1e6  # in MB\n",
    "# print(f\"Memory usage of the resulting dataframe: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del no_hw  # Delete the reference to the large object\n",
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataframes\n",
    "# Assuming the dataframes are already in memory as 'hw' and 'kg_cat'\n",
    "# If not, you would load them like this:\n",
    "# hw = pd.read_feather('/path/to/hw.feather')\n",
    "# kg_cat = pd.read_csv('/path/to/kg_cat.csv')\n",
    "\n",
    "# List of variables to average\n",
    "variables_to_average = [\n",
    "    'APPAR_TEMP', 'APPAR_TEMP_R', 'APPAR_TEMP_U', 'EFLX_LH_TOT', 'EFLX_LH_TOT_R', 'EFLX_LH_TOT_U',\n",
    "    'FGR', 'FGR_R', 'FGR_U', 'FIRA', 'FIRA_R', 'FIRA_U', 'FIRE', 'FIRE_R', 'FIRE_U',\n",
    "    'FLDS', 'FSA', 'FSA_R', 'FSA_U', 'FSDS', 'FSH', 'FSH_R', 'FSH_U', 'HEAT_FROM_AC',\n",
    "    'HIA', 'HIA_R', 'HIA_U', 'PBOT', 'Q2M', 'Q2M_R', 'Q2M_U', 'QBOT', 'Qstor',\n",
    "    'RAIN', 'SNOW', 'SOILWATER_10CM', 'TBOT', 'THBOT', 'TSA', 'TSA_R', 'TSA_U',\n",
    "    'TSKIN', 'TSKIN_R', 'TSKIN_U', 'U10', 'URBAN_HEAT', 'VAPOR_PRES', 'VAPOR_PRES_R', 'VAPOR_PRES_U',\n",
    "    'WASTEHEAT', 'WBA', 'WBA_R', 'WBA_U', 'WIND', 'UHI', 'UWBI'\n",
    "]\n",
    "\n",
    "# # Ensure 'location_ID' is of the same type in both dataframes\n",
    "# hw['location_ID'] = hw['location_ID'].astype(int)\n",
    "# kg_cat['location_ID'] = kg_cat['location_ID'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Group by KGMajorClass and local_hour, then compute averages\n",
    "grouped_df_hw = hw.groupby(['KGMajorClass', 'local_hour'])[variables_to_average].mean().reset_index()\n",
    "\n",
    "# Print the first few rows of the result\n",
    "print(grouped_df_hw)\n",
    "\n",
    "# Print the shape of the resulting dataframe\n",
    "print(f\"Shape of the resulting dataframe: {grouped_df_hw.shape}\")\n",
    "\n",
    "# # Save the result to a CSV file\n",
    "# output_path = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.02/research_results/summary/kg_class_hourly_averages.csv'\n",
    "# grouped_df.to_csv(output_path, index=False)\n",
    "# print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# # Optional: Calculate and print memory usage\n",
    "# memory_usage = grouped_df.memory_usage(deep=True).sum() / 1e6  # in MB\n",
    "# print(f\"Memory usage of the resulting dataframe: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_feather('/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.05/research_results/summary/no_hw98_kg_class_hourly_averages.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['heatwave_status'] = 'non_heatwave'\n",
    "grouped_df_hw['heatwave_status'] = 'heatwave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([grouped_df, grouped_df_hw], axis=0)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Add heatwave status column to each dataframe\n",
    "grouped_df['heatwave_status'] = 'non_heatwave'\n",
    "grouped_df_hw['heatwave_status'] = 'heatwave'\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "merged_df = pd.concat([grouped_df, grouped_df_hw], axis=0)\n",
    "\n",
    "# Move 'heatwave_status' to the front of the column list\n",
    "cols = ['heatwave_status'] + [col for col in merged_df.columns if col != 'heatwave_status']\n",
    "merged_df = merged_df[cols]\n",
    "\n",
    "# Reset the index\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "# Save as Excel file\n",
    "output_path = '/Trex/case_results/i.e215.I2000Clm50SpGs.hw_production.05/research_results/summary/merged_heatwave_data_edited.xlsx'\n",
    "merged_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "print(f\"Merged data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipJupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
